---
title: "Projet Data_mining"
author: "Axelle Barou & Amélie Picard & Tanguy Delzant"
date: "06/01/2021"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction

blabla
Nous allons tout d'abord chercher le meilleur modèle de régression logistique, puis le meileur modèle possible de reseau de neuronnes et enfin le meilleur modèle de SVM (Support Vecotr Machine).

# Présentation de la problématique et du jeu de données

Nous avons choisi un jeu de données sur Kaggle : https://www.kaggle.com/volodymyrgavrysh/bank-marketing-campaigns-dataset

Ce dernier contient 4521 observations et 17 variables.

Il s’agit d’un jeu de données sur les campagnes marketing des banques portugaises. Ces campagnes, qui se basaient sur des appels téléphoniques, permettaient d’offrir aux clients de déposer un dépôt à terme, c’est-à-dire de prêter de l’argent à une banque sur une durée fixe, avec un taux d’intérêt connu à l’avance ou variable.

Notre problématique sera donc d’essayer de prédire, en fonction des données du client, des données récoltées au cours de l’entretien téléphonique de la campagne et d’autres attributs sociaux et économiques si ces clients seront enclins à souscrire un dépôt à terme.


Voici les première lignes de notre base de données : 

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(knitr)
library(csv)
library(e1071)
library(dplyr)
library(glmnet)
library(caTools)
#le package neutralnet permet de réaliser des reseau de neurones :
library(neuralnet) 

set.seed(123)

#chargement des donnees
#setwd("C:/Users/ameli/Desktop/R/data_mining")
data<-read.csv2("bank.csv")

#Transformation de y en variable bianire : 
data$y = recode(data$y, "no" = 0, "yes" = 1)
#affichage du debut du jeu de données
data1<-data[,c(1:8)]
data2<-data[,c(8:17)]
kable(head(data1),format = "latex")
kable(head(data2),format = "latex")

```

Nous séparons les données en un échantillon d'apprentissage et un échantillon de test puis nous séparons la variable cible et les variables explicatives dans des variables différentes.
```{r}
#Echantillonage des données
ech = sort(sample(nrow(data), nrow(data)*.7))
train = data[ech, ]
test = data[-ech, ]

#Definition variables explicatives/variable cible
Xtrain = train[, -17]
ytrain = train[, 17]
Xtest = test[, -17]
ytest = test[, 17]
```

# Protocole expérimental













# Le modèle de régression logistique

La régression logistique est un modèle prédictif dont le but est de prédire/expliquer les valeurs prises par une variable cible qualitative. 
Dans notre cas, la variable cible y est de type binaire (yes/no) et désigne si le client à souscrit ou non un dépôt à terme. On parle donc de régression logistique binaire.  

Il est possible de pénaliser le modèle de régression logistique. En effet, il y a deux types de régression pénalisée:  
- la régression Ridge: pénalise la magnitude absolue des coefficients  
- la régression Lasso: pénalise le nombre de coefficient différents de 0  
Ces paramètres sont utilisés pour ajuster les coefficients de la régression à l'aide de plusieurs paramètres comme alpha qui permet d'ajuster le modèle Ridge ou Lasso et lambda qui permet de contrôler la pénalité.  

Dans un premier temps, nous allons charger et préparer les données. Nous essaierons ensuite plusieurs modèles de régression logistique, notamment des modèles pénalisés. Enfin, nous comparrons les performances de nos modèles.

## La régression logistique
Nous essayons d'abord de prédire nos données avec un modèle de régression logistique sans pénalisation (avec lambda=0).
```{r}
#Modèle de régression logistique
reg = glmnet(model.matrix(~., Xtrain), ytrain, family="binomial", lambda=0) 
#Prédiction sur les données test
predLog = predict(reg, newx = model.matrix(~., Xtest), type="class", s=c(0))
#Matrice de confusion
cm = table(predLog, ytest); cm
#Taux d'erreur
err = (cm[1,2] + cm[2,1])/sum(cm); err
```
Le taux d'erreur est de `r round(err*100,2)`% qui est donc meilleur que la prédiction par défaut.


## La régression Ridge
La régression ridge correspond à lambda>0 et alpha=0.  
Nous allons déterminer la valeur optimal de lambda par cross validation en 10 folds.
```{r}
# Optimisation du paramètre lambda
cv.ridge = cv.glmnet(model.matrix(~., Xtrain), ytrain, family="binomial", type.measure="class", nfolds=10, alpha=0, keep=TRUE)
plot(cv.ridge)
```

Le graphique met en relation les valeurs de log(lambda) avec le taux d’erreur moyen en validation croisée.  
Le premier trait en pointillé représente le lambda qui minimise l'erreur. Il vaut `r round(log(cv.ridge$lambda.min),2)`.  
Le deuxième trait en pointillé représente la plus grande valeur de lambda pour laquelle l'erreur moyenne en validation croisée est inférieure à la borne haute de l’intervalle de confiance de l’erreur optimale. Il vaut `r round(log(cv.ridge$lambda.1se),2)`.  

```{r}
lambda_min = cv.ridge$lambda.min
lambda_1se = cv.ridge$lambda.1se
```


Nous allons essayer de faire les prédictions avec ces deux valeurs de lambda.
```{r}
#Prédiction
pred_ridge = predict(cv.ridge , newx = model.matrix(~., Xtest), s=c(lambda_min, lambda_1se), type="class")
#Matrices de confusion
cm_ridge1 = table(pred_ridge[,1], ytest)
cm_ridge2 = table(pred_ridge[,2], ytest)
#Taux d'erreur
err_ridge1 = (cm_ridge1[1,2] + cm_ridge1[2,1])/sum(cm_ridge1); err_ridge1
err_ridge2 = (cm_ridge2[1,2] + cm_ridge2[2,1])/sum(cm_ridge2); err_ridge2
```
Les deux modèles Ridge font mieux que la prédiction par défaut mais sont moins bien que le modele precedent (sans pénalisation).  
Si on compare les deux modèles de Ridge, le premier est un peu meilleur. Le modèle le plus pénalisé n'est donc pas le meilleur choix dans notre cas.

## La régression elasticnet
La régression ridge correspond à lambda>0 et alpha>0.  
Nous allons déterminer la valeur optimal de lambda et alpha.  
Nous commençons par cross valider notre modèle.
```{r}
#Modèles
for (i in 1:10) {
  assign(paste("fit", i, sep=""), cv.glmnet(model.matrix(~., Xtrain), ytrain, type.measure="class", alpha=i/10,family="binomial"))
}
```
  
Prédiction avec les différents modèles
```{r}
#Predictions
pred_enet1 = predict(fit1, s=c(fit1$lambda.1se, fit1$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet2 = predict(fit2, s=c(fit2$lambda.1se, fit2$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet3 = predict(fit3, s=c(fit3$lambda.1se, fit3$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet4 = predict(fit4, s=c(fit4$lambda.1se, fit4$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet5 = predict(fit5, s=c(fit5$lambda.1se, fit5$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet6 = predict(fit6, s=c(fit6$lambda.1se, fit6$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet7 = predict(fit7, s=c(fit7$lambda.1se, fit7$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet8 = predict(fit8, s=c(fit8$lambda.1se, fit8$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet9 = predict(fit9, s=c(fit9$lambda.1se, fit9$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet10 = predict(fit10, s=c(fit10$lambda.1se, fit10$lambda.min), newx=model.matrix(~., Xtest), type="class")
```

Calcul des matrices de confusion
```{r}
#Matrices de confusion
cm_enet1_1se = table(pred_enet1[,1], ytest)
cm_enet2_1se = table(pred_enet2[,1], ytest)
cm_enet3_1se = table(pred_enet3[,1], ytest)
cm_enet4_1se = table(pred_enet4[,1], ytest)
cm_enet5_1se = table(pred_enet5[,1], ytest)
cm_enet6_1se = table(pred_enet6[,1], ytest)
cm_enet7_1se = table(pred_enet7[,1], ytest)
cm_enet8_1se = table(pred_enet8[,1], ytest)
cm_enet9_1se = table(pred_enet9[,1], ytest)
cm_enet10_1se = table(pred_enet10[,1], ytest)

cm_enet1_min = table(pred_enet1[,2], ytest)
cm_enet2_min = table(pred_enet2[,2], ytest)
cm_enet3_min = table(pred_enet3[,2], ytest)
cm_enet4_min = table(pred_enet4[,2], ytest)
cm_enet5_min = table(pred_enet5[,2], ytest)
cm_enet6_min = table(pred_enet6[,2], ytest)
cm_enet7_min = table(pred_enet7[,2], ytest)
cm_enet8_min = table(pred_enet8[,2], ytest)
cm_enet9_min = table(pred_enet9[,2], ytest)
cm_enet10_min = table(pred_enet10[,2], ytest)
```
  
Calcul des taux d'erreur
```{r}
#Taux d'erreur
err_enet1_1se = (cm_enet1_1se[1,2] + cm_enet1_1se[2,1])/sum(cm_enet1_1se)
err_enet2_1se = (cm_enet2_1se[1,2] + cm_enet2_1se[2,1])/sum(cm_enet2_1se)
err_enet3_1se = (cm_enet3_1se[1,2] + cm_enet3_1se[2,1])/sum(cm_enet3_1se)
err_enet4_1se = (cm_enet4_1se[1,2] + cm_enet4_1se[2,1])/sum(cm_enet4_1se)
err_enet5_1se = (cm_enet5_1se[1,2] + cm_enet5_1se[2,1])/sum(cm_enet5_1se)
err_enet6_1se = (cm_enet6_1se[1,2] + cm_enet6_1se[2,1])/sum(cm_enet6_1se)
err_enet7_1se = (cm_enet7_1se[1,2] + cm_enet7_1se[2,1])/sum(cm_enet7_1se)
err_enet8_1se = (cm_enet8_1se[1,2] + cm_enet8_1se[2,1])/sum(cm_enet8_1se)
err_enet9_1se = (cm_enet9_1se[1,2] + cm_enet9_1se[2,1])/sum(cm_enet9_1se)
err_enet10_1se = (cm_enet10_1se[1,2] + cm_enet10_1se[2,1])/sum(cm_enet10_1se)
err_enet_1se = c(err_enet1_1se, err_enet2_1se, err_enet3_1se, err_enet4_1se, err_enet5_1se, err_enet6_1se, err_enet7_1se, err_enet8_1se, err_enet9_1se, err_enet10_1se)

err_enet1_min = (cm_enet1_min[1,2] + cm_enet1_min[2,1])/sum(cm_enet1_min)
err_enet2_min = (cm_enet2_min[1,2] + cm_enet2_min[2,1])/sum(cm_enet2_min)
err_enet3_min = (cm_enet3_min[1,2] + cm_enet3_min[2,1])/sum(cm_enet3_min)
err_enet4_min = (cm_enet4_min[1,2] + cm_enet4_min[2,1])/sum(cm_enet4_min)
err_enet5_min = (cm_enet5_min[1,2] + cm_enet5_min[2,1])/sum(cm_enet5_min)
err_enet6_min = (cm_enet6_min[1,2] + cm_enet6_min[2,1])/sum(cm_enet6_min)
err_enet7_min = (cm_enet7_min[1,2] + cm_enet7_min[2,1])/sum(cm_enet7_min)
err_enet8_min = (cm_enet8_min[1,2] + cm_enet8_min[2,1])/sum(cm_enet8_min)
err_enet9_min = (cm_enet9_min[1,2] + cm_enet9_min[2,1])/sum(cm_enet9_min)
err_enet10_min = (cm_enet10_min[1,2] + cm_enet10_min[2,1])/sum(cm_enet10_min)
err_enet_min = c(err_enet1_min, err_enet2_min, err_enet3_min, err_enet4_min, err_enet5_min, err_enet6_min, err_enet7_min, err_enet8_min, err_enet9_min, err_enet10_min)
```
  
Comparaion des taux d'erreur
```{r}
df_err_enet = data.frame(err_enet_1se,err_enet_min)
df_err_enet
```
Nous remarquons que les modèles les plus pénalisés (ceux avec lambda.1se) ont un taux d'erreur plus elevé et sont donc moins bon.
Le meilleur de nos modèles a un taux d'erreur de `r min(df_err_enet)`.

Récupérons à présents nos 3 meilleurs modèles.
Nous allons afficher la courbe ROC afin de comparer ces modèles.
```{r}
all_pred = data.frame(as.numeric(predLog), as.numeric(pred_ridge[,2]), as.numeric(pred_enet8[,2]))
colnames(all_pred) = c("reg_log", "reg_ridge", "reg_enet")
colAUC(all_pred, ytest, plotROC = TRUE)
abline(0,1, col = "blue")

```
  
Nous remarquons que nos trois modèles font mieux que l'aléatoire. Le meilleur des trois modèles est le modèle non pénalisé suivi du modèle elasticnet puis du modèle Ridge.  
Les aires sous la courbes sont compris entre `r round(min(colAUC(all_pred, ytest,)),2)` et `r round(max(colAUC(all_pred, ytest,)),2)`. Il ne s'agit donc pas d'exellents modèles.

# Réalisation du réseau de neurones

Nous allons voir le protocole expérimentale, puis nous allons le décrire plus en détail le déroulement de l'analyse réalisé ainsi que les conclusions.

## Transformation des données

Avant de réaliser les réseaux de neurones, nous devons transformer les données. Pour les variables quantitatives, nous allons les normaliser pour qu'elle ai la même importance. Pour les variables qualitatives il faut les convertir en ordonné en leur donnant une valeur numeric de 1 à k ou k est le nombre de facteur. 
Avant de paramètrer le réseau de neuronne, il faut normaliser les données, les variables explicatives, dans notre cas toutes les variables du jeu de données sauf "y" la variables cible.

```{r message=FALSE, warning=FALSE, echo=FALSE}
#Echantillonage des données
ech = sort(sample(nrow(data), nrow(data)*.7))
train = data[ech, ]
test = data[-ech, ]

#Definition variables explicatives/variable cible
Xtrain = train[, -17]
ytrain = train[, 17]
Xtest = test[, -17]
ytest = test[, 17]

#on récupère les données de base
trainRN<-train
testRN<-test
testquali<-test[,c(2,3,4,5,7,8,9,11,16)]
trainquali<-train[,c(2,3,4,5,7,8,9,11,16)]

#quali en quali numeric
#fonction qui change pour une variable les noms des modalité par un nombre :
encoding<-function(var) {
    cpt=0
    for (i in levels(as.factor(var))) {
        for (j in length(var)) {
            var[var==i]<-cpt
        }
        cpt=cpt+1

    }
    return(as.numeric(var))
}
trainquali<-lapply(trainquali, encoding)
testquali<-lapply(testquali, encoding)

#donnée avec toute les variables numérique :
trainRN[,c(2,3,4,5,7,8,9,11,16)]<-trainquali
testRN[,c(2,3,4,5,7,8,9,11,16)]<-testquali

#normalisation des variables 
trainRN<-data.frame(scale(trainRN[-17]),trainRN[17])
testRN<-data.frame(scale(testRN[-17]),testRN[17])
```

De plus, pour la vérification et la validation du meilleur reseau de neurone, nous découpons nos données en trois échantillons. A partir du premier découpage dans les données pour obtenir un échantillon d'apprentissage qui est constitué de 80% de la base et un échantillon test de 20% de la base. Nous re-découpons l'échantillon d'apprentissage en deux, pour récupérer un échantillon de validation qui est constitué de 30% de l'échantillon d'apprentissage, et donc nous utiliserons un échantillon d'apprentissage un peu plus petit uniquement pour la partie réseau de neurone. 

```{r}
#Découpage de l'échantillon d'apprentissage en ech de validation : 
#Echantillonage des données
ech2 = sort(sample(nrow(trainRN), nrow(trainRN)*.7))

valid = trainRN[-ech2, ]
trainRN = trainRN[ech2, ]

#Definition variables explicatives/variable cible 
XtrainRN = trainRN[, -17]
ytrainRN = trainRN[, 17]
Xvalid = valid[, -17]
yvalid = valid[, 17]
XtestRN = testRN[, -17]
ytestRN = testRN[, 17]

```


## Paramétrage du réseau de neurone

Nous allons réaliser un réseau de neurones à l'aide de la fonction "neutralnet" qui permet de réaliser tout type de réseau de neurones.
Nous allons paramétrer cette méthode grâce à de nombreux paramètres : 
 - hidden : spécifie le nombre de neuronnes dans la couche caché
 - err.fct : fonction utilisé pour determiner le calcul de l'erreur (ce : entropie croisée, sse : somme des erreurs au carré)
 - linear.output = FALSE : Ne pas réaliser de regression linéaire
 - algorithm : contient une chaine définissant le type d'algorithme, par défaut vaut "rprop+" qui signifie rétropropagation résiliente avec retour de poids
 - act.fct : permet de lisser le résultat du produit croisé des neurones et des poids, par défaut "logistic" qui se rapporte à la fonction logistique.

Cette fonction utilise d'autre paramètres comme : 
 - startweights : contient la valeurs de départ des poids (par défaut une initialisation aléatoire)
 - rep : Nombre d'entrainement du réseau (pas utiliser pour éviter le sur-apprentissage)

Dans le cas d'une variable y catégorielle binaire, nous choisissons comme fonction d'erreur l'entropie croissée.

Premier modéle : 

```{r}
nn<-neuralnet(ytrainRN~., data = XtrainRN, hidden=10, err.fct="ce",linear.output = FALSE)
#attributes(nn)
#nn$result.matrix

```

Validation du modèle : 

```{r}
prob <- predict(nn,valid)
pred <- ifelse(prob>0.5, 1, 0)

```

Voici la matrice de confusion entre les valeurs prédite et les valeurs test et le taux d'erreur :  
```{r}
m<-table(pred,yvalid)
m
#tx d'erreur :
tx<-(m[1,2]+m[2,1])/length(yvalid)
tx
```

Le taux d'erreur de ce premier modéle est de `r round(tx*100,2)`%.

Deuxième modèle : 
```{r}
nn2=neuralnet(ytrainRN~., data = XtrainRN, hidden=10, err.fct="ce",linear.output = FALSE,likelihood = TRUE)

prob2 <- predict(nn2,valid)
pred2 <- ifelse(prob2>0.5, 1, 0)

m2<-table(pred2,yvalid)
tx2<-(m2[1,2]+m2[2,1])/length(yvalid)
tx2

```
Le modéle deux est moins performant avec le "likelihood" (taux d'erreur = `r round(tx*100,2)`% > `r round(tx2*100,2)`%).

Troixième modèle : augmentation du nombre de couche de la couche caché de 10 à 17 (nombre de variable d'entrée)
```{r}
nn3=neuralnet(ytrainRN~., data = XtrainRN, hidden=17, err.fct="ce",linear.output = FALSE)

prob3 <- predict(nn3,valid)
pred3 <- ifelse(prob3>0.5, 1, 0)

m3<-table(pred3,yvalid)
tx3<-(m3[1,2]+m3[2,1])/length(yvalid)
tx3

```

Le taux d'erreur est plus faible que pour une couche comprenant 10 neurones cachés (`r round(tx3*100,2)`%)

Quatrième modèle : augmentation du nombre de couche de la couche caché de 17 à 34 (deux fois le nombre de variable d'entrée)
```{r}
nn4=neuralnet(ytrainRN~., data = XtrainRN, hidden=34, err.fct="ce",linear.output = FALSE)

prob4 <- predict(nn4,valid)
pred4 <- ifelse(prob4>0.5, 1, 0)

m4<-table(pred4,yvalid)
tx4<-(m4[1,2]+m[2,1])/length(yvalid)
tx4

```

Taux d'erreur un peu élevé, donc moins intéressant (`r round(tx4*100,2)`%)

On garde la prédiction n°3 pour le nombre de neurones de la couche caché. Et nous décidons de garder 17 neurones dans la couche caché.

Cinquième modèle : On utilise une couche caché composé de 34 neurones et on passe l'argument "likelihood" à vrai.
```{r}
nn5<-neuralnet(ytrainRN~., data = XtrainRN, hidden=34, err.fct="ce",linear.output = FALSE, likelihood = TRUE)

prob5 <- predict(nn5,valid)
pred5 <- ifelse(prob5>0.5, 1, 0)

m5<-table(pred5,yvalid)
tx5<-(m5[1,2]+m[2,1])/length(yvalid)
tx5

```

Cette fois ci le taux d'erreur est plus élevé (`r round(tx5*100,2)`%)

Nous testons un dernier modéle.

Sixième modèle : Nous augumentons encore une fois le nombre de neurones de la couche cachée (3 fois le nombre de variable d'entrée)
```{r}
nn6<-neuralnet(ytrainRN~., data = XtrainRN, hidden=51, err.fct="ce",linear.output = FALSE, likelihood = TRUE)

prob6 <- predict(nn6,valid)
pred6 <- ifelse(prob6>0.5, 1, 0)

m6<-table(pred6,yvalid)
tx6<-(m6[1,2]+m[2,1])/length(yvalid)

```
Cette fois le taux d'erreur est de `r round(tx6*100,2)`% ce qui est faiblement plus élevé que pour le quatrième modèle (`r round(tx4*100,2)`%). Mais ce modèle utilise un plus grand nombre de neurone ce qui peut engendrer du sur-apprentissage. 

Avec la library "neutralnn" et la variable cible y correspondant à une classification binaire, il n'est pas possible de paramétrer le réseau de neurone à l'infinie. Les paramétrages pour une telle variable sont faible. Le paramètre "algorithme" ne fonctionne qu'avec "rprop +", la fonction d'erreur ne peut pas être modifier et la demande d'une régression logistique est impossible. Nous aurions pu changer les poids les variables d'entrée, mais le choix de l'aléatoire nous pariassait plus juste.
Néanmoins, avec les réseaux de neurones que nous avons construit nous avons un taux d'erreur plutôt faible, ce qui nous paraissait être un bon résultat.Pour confirmer cette avis, nous décidons de comparer les courbes ROC des six modèles.

courbe ROC : 
```{r}
all_pred = data.frame(pred, pred2,  pred4, pred5, pred6)
colnames(all_pred) = c("nn","nn2","nn4","nn5","nn6")
colAUC(all_pred, yvalid, plotROC = TRUE)
abline(0,1, col = "blue")

```

L'air sous la courbe des Courbe ROC sous toutes supérieure à la courbe aléatoire, ce qui signifie que tous nos modèles sont meilleurs que l'aléatoire.
A l'aide de la Courbe ROC ainsi que du taux d'erreur, nous considérons le meilleur réseau de neurone étant le quatrième modèle construit. C'est a dire que nous utilisons 17 neuronnes caché et que nous plaçons à vrai l'argument "likelihood". C'est avec ce modèle que nous allons vérifier la bonne prédiction des valeurs test.

## Prédiction de la variable cible y

Représentation graphique du réseau de neurones séléctionné : 

```{r}
plot(nn4)

```

Voici la distribution de la variable y prédite par le réseau de neurone :

```{r}
prob <- predict(nn4,testRN)
predRN <- ifelse(prob>0.5, 1, 0)

```

Voici la matrice de confusion entre les valeurs prédite et les valeurs test : 
```{r}
mRN<-table(predRN,ytestRN)
kable(mRN)
```

# SVM

Nous utilisons le package ’e1071’ pour l’implémentation des SVM. Nous demandons à la procédure svm() de construire un classifieur dont on ne centre pas les valeurs avec un noyau de type sigmoid.
On applique cette fonction à nos données d'apprentissage
```{r}
model <- svm(model.matrix(~., Xtrain), ytrain, scale=F, type= "C-classification",kernel='sigmoid')
summary(model)
```

On obtient 692 points supports ce qui est par rapport à la taille d'échantillon (3164 observations) n'est pas forcément trop élevé, donc la modélisation est efficace


```{r}
#Prédiction sur les données test
pred = predict(model, newdata = model.matrix(~., Xtest))
#Matrice de confusion
cm = table(pred, ytest); cm
#Taux d'erreur
err = (cm[1,2] + cm[2,1])/sum(cm); err
```
Le taux d'erreur est de `r round(err*100,2)`% qui est donc moins bonne que la prédiction par défaut. (11.52%)

Ce model n'ayant pas été tuné, il n'est que la version simple de ce que nous pouvons avoir.
Maintenant, nous allons essayer d'ajuster notre modèle en tunant pour le moment deux hyperparamètres : C et Gamma.

Pour rappel, l'hyperparamètre C est responsable de la taille de la marge du MVC. Cela signifie que les points situés à l'intérieur de cette marge ne sont classés dans aucune des deux catégories.  Plus la valeur de C est faible, plus la marge est importante

L'hyperparamètre gamma doit être réglé pour mieux adapter l'hyperplan aux données.  Il est responsable du degré de linéarité de l'hyperplan, et pour cela, il n'est pas présent lors de l'utilisation de noyaux linéaires. Plus γ est petit, plus l'hyperplan aura l'air d'une ligne droite, tandis que si γ est trop grand, l'hyperplan sera plus courbé et pourrait trop bien délimiter les données, ce qui entraînerait un overfitting.


## Optimisation des hyperparamètres
```{r}
#Optimisation de Gamma et C
tuned = tune.svm(x=model.matrix(~., Xtrain),
                 y=as.factor(ytrain), 
                 scale=F, type = "C-classification", kernel='sigmoid',
                 cost = 10^(-1:2), 
                 gamma = c(0.1, 1, 10),
                 tunecontrol=tune.control(cross=10))
tuned$performances
```

Le tableau met en évidence les différentes valeurs de gamma et C testées avec le taux d'erreur moyen pour 10 validations.
On constate donc que 3 modèles sortent du lot avec des taux d'erreur presque similaire au alentour de 12.8% :
gamma : 0.1 & cost : 0.1
gamma : 1.0 & cost : 0.1
gamma : 10.0 & cost : 0.1

```{r}
plot(tuned)
```

Nous allons donc faire nos prédictions avec deux modèles, étant donné que nous avons le même taux d'erreur pour deux modèles, nous prendrons donc celui recommendé par l'algorithme.


```{r}
svmfit = tuned$best.model
#Prédiction sur les données test
pred2 = predict(svmfit, newdata = model.matrix(~., Xtest))
#Matrice de confusion
cm = table(pred2, ytest); cm
#Taux d'erreur
err = (cm[1,2] + cm[2,1])/sum(cm); err
```

```{r}
model2=svm(model.matrix(~., Xtrain), ytrain, scale=F,cost=0.1, gamma=0.1, type= "C-classification",kernel='sigmoid')
#Prédiction sur les données test
pred3 = predict(model2, newdata = model.matrix(~., Xtest))
#Matrice de confusion
cm = table(pred3, ytest); cm
#Taux d'erreur
err = (cm[1,2] + cm[2,1])/sum(cm); err
```
On constate donc que nous avons le même taux d'erreur. Cependant, nous avons une moins bonne prédiction que celle par défaut (13.19% contre 11.52%)

Cependant, nous avons essayé que peu de valeur pour nos hyperparamètres et avons obtenu de bons résultats, on va donc élargir notre champ de valeur possible pour C et Gamma, pour essayer d'obtenir de meilleur résultat tout en évitant de l'overfitting.

## Optimisation des hyperparamètres
```{r}
#Optimisation de Gamma et C
tuned2 = tune.svm(x=model.matrix(~., Xtrain),
                 y=as.factor(ytrain), 
                 scale=F, type = "C-classification", kernel='sigmoid',
                 cost = 10^(-1:3), 
                 gamma = 10^(-5:-1),
                 tunecontrol=tune.control(cross=10))
tuned2$performances
```

On a augmenté le champ de valeur possible pour C et diminué celui de gamma, car c'est ce qui correspond le mieux pour avoir le meilleur modèle possible. Au vu des différents taux d'erreur, nous n'avons pas de meilleurs résultats comparé à la précédente cross validation. On pourrait donc supposer que notre meilleur modèle pour ce jeu de données seraient avec C = 0.1 et gamma = 0.1


Nous allons maintenant afficher la courbe ROC de nos deux modèles à comparer

```{r}
pred=as.numeric(pred)
predSVM=recode(pred, '1'=0, '2'=1)
pred2=as.numeric(pred2)
pred2=recode(pred2, '1'=0, '2'=1)
```

```{r}
all_pred = data.frame(predSVM, pred2)
colnames(all_pred) = c("svm_W/O_cv", "svm_w/_cv")
colAUC(all_pred, ytest, plotROC = TRUE)
abline(0,1, col = "blue")
```
On remarque que parmi nos 2 modèles, un fait mieux que celui aléatoire et l'autre le surpasse très légèrement
Les aires sous la courbes sont compris entre `r round(min(colAUC(all_pred, ytest,)),2)` et `r round(max(colAUC(all_pred, ytest,)),2)`.
Ces deux modèles ne sont donc pas performant.

# Comparaison des 3 modèles séléctionnée

Nous allons maintenant comparer les trois modèles séléctionné pour chaque méthodes. Puis nous choisirons le plus efficasse de tous.

```{r}
#Matrice de confusion
cm = table(predLog, ytest)
#Taux d'erreur
errLog = (cm[1,2] + cm[2,1])/sum(cm)

#tx d'erreur reseau de neurone:
txRN<-(mRN[1,2]+mRN[2,1])/length(ytest)
#SVM :
#Matrice de confusion
cm = table(predSVM, ytest)
#Taux d'erreur
errSVM = (cm[1,2] + cm[2,1])/sum(cm)

```
Le taux d'erreur du modèle de la regression logistique est de `r round(errLog*100,2)`%, celui du réseau de neurones est `r round(txRN*100,2)`% et enfin celui du SVM est `r round(errSVM*100,2)`%

courbe ROC représentant : 
```{r}
all_pred<-data.frame(as.numeric(predLog), as.numeric(predRN), as.numeric(predSVM))
colnames(all_pred)=c("Régression logistique","Reseau de neurones","SVM")
colAUC(all_pred, ytest, plotROC = TRUE)
abline(0,1, col = "blue")

```



# Conclusion

Pour conclure, grâce à ce projet nous avons pu apprendre à paramètrer des modèles de machine learning sous R, ainsi que créer des rapports sous Rmarkdown. 

Néanmoins nous avons rencontré quelques difficulté durant ce projet. En effet le travail à distance ne nous a pas permis de travailler ensemble mais d'échanger par message ou visio. De plus lors du paramétrage des réseaux de neurones, nous nous sommes rendu compte que la library "neutralnet" ne laisse pas la possibilité de changer beaucoup de paramètres car la variable cible est catégorielle et bianaire.

Pour aller plus loin, nous pourons chercher une autre library pour les réseaux de neurones, pour obtenir un meilleur modèle.




