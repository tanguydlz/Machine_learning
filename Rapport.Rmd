---
title: "Projet Data_mining"
author: "Axelle Barou & Amélie Picard & Tanguy Delzant"
date: "06/01/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction

blabla
Nous allons tout d'abord chercher le meilleur modèle de régression logistique, puis le meileur modèle possible de reseau de neuronnes et enfin le meilleur modèle de SVM (Support Vecotr Machine).

# Présentation de la problématique et du jeu de données

Nous avons choisi un jeu de données sur Kaggle : https://www.kaggle.com/volodymyrgavrysh/bank-marketing-campaigns-dataset

Ce dernier contient 41188 observations et 21 variables.

Il s’agit d’un jeu de données sur les campagnes marketing des banques portugaises. Ces campagnes, qui se basaient sur des appels téléphoniques, permettaient d’offrir aux clients de déposer un dépôt à terme, c’est-à-dire de prêter de l’argent à une banque sur une durée fixe, avec un taux d’intérêt connu à l’avance ou variable.

Notre problématique sera donc d’essayer de prédire, en fonction des données du client, des données récoltées au cours de l’entretien téléphonique de la campagne et d’autres attributs sociaux et économiques si ces clients seront enclins à souscrire un dépôt à terme.


Voici les première lignes de notre base de données : 

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(knitr)
library(csv)
library(e1071)
library(dplyr)
library(glmnet)
library(caTools)
#le package neutralnet permet de réaliser des reseau de neurones :
library(neuralnet) 

#chargement des donnees
#setwd("C:/Users/ameli/Desktop/R/data_mining")
data<-read.csv2("bank.csv")

#Transformation de y en variable bianire : 
data$y = recode(data$y, "no" = 0, "yes" = 1)
#affichage du debut du jeu de données
data1<-data[,c(1:8)]
data2<-data[,c(8:17)]
kable(head(data1),format = "latex")
kable(head(data2),format = "latex")

```

# Le modèle de régression logistique

La régression logistique est un modèle prédictif dont le but est de prédire/expliquer les valeurs prises par une variable cible qualitative. 
Dans notre cas, la variable cible y est de type binaire (yes/no) et désigne si le client à souscrit ou non un dépôt à terme. On parle donc de régression logistique binaire.  

Il est possible de pénaliser le modèle de régression logistique. En effet, il y a deux types de régression pénalisée:  
- la régression Ridge: pénalise la magnitude absolue des coefficients  
- la régression Lasso: pénalise le nombre de coefficient différents de 0  
Ces paramètres sont utilisés pour ajuster les coefficients de la régression à l'aide de plusieurs paramètres comme alpha qui permet d'ajuster le modèle Ridge ou Lasso et lambda qui permet de contrôler la pénalité.  

Dans un premier temps, nous allons charger et préparer les données. Nous essaierons ensuite plusieurs modèles de régression logistique, notamment des modèles pénalisés. Enfin, nous comparrons les performances de nos modèles.

## La régression logistique
Nous essayons d'abord de prédire nos données avec un modèle de régression logistique sans pénalisation (avec lambda=0).
```{r}
#Modèle de régression logistique
reg = glmnet(model.matrix(~., Xtrain), ytrain, family="binomial", lambda=0) 
#Prédiction sur les données test
predLog = predict(reg, newx = model.matrix(~., Xtest), type="class", s=c(0))
#Matrice de confusion
cm = table(predLog, ytest); cm
#Taux d'erreur
err = (cm[1,2] + cm[2,1])/sum(cm); err
```
Le taux d'erreur est de `r round(err*100,2)`% qui est donc meilleur que la prédiction par défaut.


## La régression Ridge
La régression ridge correspond à lambda>0 et alpha=0.  
Nous allons déterminer la valeur optimal de lambda par cross validation en 10 folds.
```{r}
# Optimisation du paramètre lambda
cv.ridge = cv.glmnet(model.matrix(~., Xtrain), ytrain, family="binomial", type.measure="class", nfolds=10, alpha=0, keep=TRUE)
plot(cv.ridge)
```

Le graphique met en relation les valeurs de log(lambda) avec le taux d’erreur moyen en validation croisée.  
Le premier trait en pointillé représente le lambda qui minimise l'erreur. Il vaut `r round(log(cv.ridge$lambda.min),2)`.  
Le deuxième trait en pointillé représente la plus grande valeur de lambda pour laquelle l'erreur moyenne en validation croisée est inférieure à la borne haute de l’intervalle de confiance de l’erreur optimale. Il vaut `r round(log(cv.ridge$lambda.1se),2)`.  

```{r}
lambda_min = cv.ridge$lambda.min
lambda_1se = cv.ridge$lambda.1se
```


Nous allons essayer de faire les prédictions avec ces deux valeurs de lambda.
```{r}
#Prédiction
pred_ridge = predict(cv.ridge , newx = model.matrix(~., Xtest), s=c(lambda_min, lambda_1se), type="class")
#Matrices de confusion
cm_ridge1 = table(pred_ridge[,1], ytest)
cm_ridge2 = table(pred_ridge[,2], ytest)
#Taux d'erreur
err_ridge1 = (cm_ridge1[1,2] + cm_ridge1[2,1])/sum(cm_ridge1); err_ridge1
err_ridge2 = (cm_ridge2[1,2] + cm_ridge2[2,1])/sum(cm_ridge2); err_ridge2
```
Les deux modèles Ridge font mieux que la prédiction par défaut mais sont moins bien que le modele precedent (sans pénalisation).  
Si on compare les deux modèles de Ridge, le premier est un peu meilleur. Le modèle le plus pénalisé n'est donc pas le meilleur choix dans notre cas.

## La régression elasticnet
La régression ridge correspond à lambda>0 et alpha>0.  
Nous allons déterminer la valeur optimal de lambda et alpha.  
Nous commençons par cross valider notre modèle.
```{r}
#Modèles
for (i in 1:10) {
  assign(paste("fit", i, sep=""), cv.glmnet(model.matrix(~., Xtrain), ytrain, type.measure="class", alpha=i/10,family="binomial"))
}
```
  
Prédiction avec les différents modèles
```{r}
#Predictions
pred_enet1 = predict(fit1, s=c(fit1$lambda.1se, fit1$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet2 = predict(fit2, s=c(fit2$lambda.1se, fit2$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet3 = predict(fit3, s=c(fit3$lambda.1se, fit3$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet4 = predict(fit4, s=c(fit4$lambda.1se, fit4$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet5 = predict(fit5, s=c(fit5$lambda.1se, fit5$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet6 = predict(fit6, s=c(fit6$lambda.1se, fit6$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet7 = predict(fit7, s=c(fit7$lambda.1se, fit7$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet8 = predict(fit8, s=c(fit8$lambda.1se, fit8$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet9 = predict(fit9, s=c(fit9$lambda.1se, fit9$lambda.min), newx=model.matrix(~., Xtest), type="class")
pred_enet10 = predict(fit10, s=c(fit10$lambda.1se, fit10$lambda.min), newx=model.matrix(~., Xtest), type="class")
```

Calcul des matrices de confusion
```{r}
#Matrices de confusion
cm_enet1_1se = table(pred_enet1[,1], ytest)
cm_enet2_1se = table(pred_enet2[,1], ytest)
cm_enet3_1se = table(pred_enet3[,1], ytest)
cm_enet4_1se = table(pred_enet4[,1], ytest)
cm_enet5_1se = table(pred_enet5[,1], ytest)
cm_enet6_1se = table(pred_enet6[,1], ytest)
cm_enet7_1se = table(pred_enet7[,1], ytest)
cm_enet8_1se = table(pred_enet8[,1], ytest)
cm_enet9_1se = table(pred_enet9[,1], ytest)
cm_enet10_1se = table(pred_enet10[,1], ytest)

cm_enet1_min = table(pred_enet1[,2], ytest)
cm_enet2_min = table(pred_enet2[,2], ytest)
cm_enet3_min = table(pred_enet3[,2], ytest)
cm_enet4_min = table(pred_enet4[,2], ytest)
cm_enet5_min = table(pred_enet5[,2], ytest)
cm_enet6_min = table(pred_enet6[,2], ytest)
cm_enet7_min = table(pred_enet7[,2], ytest)
cm_enet8_min = table(pred_enet8[,2], ytest)
cm_enet9_min = table(pred_enet9[,2], ytest)
cm_enet10_min = table(pred_enet10[,2], ytest)
```
  
Calcul des taux d'erreur
```{r}
#Taux d'erreur
err_enet1_1se = (cm_enet1_1se[1,2] + cm_enet1_1se[2,1])/sum(cm_enet1_1se)
err_enet2_1se = (cm_enet2_1se[1,2] + cm_enet2_1se[2,1])/sum(cm_enet2_1se)
err_enet3_1se = (cm_enet3_1se[1,2] + cm_enet3_1se[2,1])/sum(cm_enet3_1se)
err_enet4_1se = (cm_enet4_1se[1,2] + cm_enet4_1se[2,1])/sum(cm_enet4_1se)
err_enet5_1se = (cm_enet5_1se[1,2] + cm_enet5_1se[2,1])/sum(cm_enet5_1se)
err_enet6_1se = (cm_enet6_1se[1,2] + cm_enet6_1se[2,1])/sum(cm_enet6_1se)
err_enet7_1se = (cm_enet7_1se[1,2] + cm_enet7_1se[2,1])/sum(cm_enet7_1se)
err_enet8_1se = (cm_enet8_1se[1,2] + cm_enet8_1se[2,1])/sum(cm_enet8_1se)
err_enet9_1se = (cm_enet9_1se[1,2] + cm_enet9_1se[2,1])/sum(cm_enet9_1se)
err_enet10_1se = (cm_enet10_1se[1,2] + cm_enet10_1se[2,1])/sum(cm_enet10_1se)
err_enet_1se = c(err_enet1_1se, err_enet2_1se, err_enet3_1se, err_enet4_1se, err_enet5_1se, err_enet6_1se, err_enet7_1se, err_enet8_1se, err_enet9_1se, err_enet10_1se)

err_enet1_min = (cm_enet1_min[1,2] + cm_enet1_min[2,1])/sum(cm_enet1_min)
err_enet2_min = (cm_enet2_min[1,2] + cm_enet2_min[2,1])/sum(cm_enet2_min)
err_enet3_min = (cm_enet3_min[1,2] + cm_enet3_min[2,1])/sum(cm_enet3_min)
err_enet4_min = (cm_enet4_min[1,2] + cm_enet4_min[2,1])/sum(cm_enet4_min)
err_enet5_min = (cm_enet5_min[1,2] + cm_enet5_min[2,1])/sum(cm_enet5_min)
err_enet6_min = (cm_enet6_min[1,2] + cm_enet6_min[2,1])/sum(cm_enet6_min)
err_enet7_min = (cm_enet7_min[1,2] + cm_enet7_min[2,1])/sum(cm_enet7_min)
err_enet8_min = (cm_enet8_min[1,2] + cm_enet8_min[2,1])/sum(cm_enet8_min)
err_enet9_min = (cm_enet9_min[1,2] + cm_enet9_min[2,1])/sum(cm_enet9_min)
err_enet10_min = (cm_enet10_min[1,2] + cm_enet10_min[2,1])/sum(cm_enet10_min)
err_enet_min = c(err_enet1_min, err_enet2_min, err_enet3_min, err_enet4_min, err_enet5_min, err_enet6_min, err_enet7_min, err_enet8_min, err_enet9_min, err_enet10_min)
```
  
Comparaion des taux d'erreur
```{r}
df_err_enet = data.frame(err_enet_1se,err_enet_min)
df_err_enet
```
Nous remarquons que les modèles les plus pénalisés (ceux avec lambda.1se) ont un taux d'erreur plus elevé et sont donc moins bon.
Le meilleur de nos modèles a un taux d'erreur de `r min(df_err_enet)`.

Récupérons à présents nos 3 meilleurs modèles.
Nous allons afficher la courbe ROC afin de comparer ces modèles.
```{r}
all_pred = data.frame(as.numeric(predLog), as.numeric(pred_ridge[,2]), as.numeric(pred_enet8[,2]))
colnames(all_pred) = c("reg_log", "reg_ridge", "reg_enet")
colAUC(all_pred, ytest, plotROC = TRUE)
abline(0,1, col = "blue")

```
  
Nous remarquons que nos trois modèles font mieux que l'aléatoire. Le meilleur des trois modèles est le modèle non pénalisé suivi du modèle elasticnet puis du modèle Ridge.  
Les aires sous la courbes sont compris entre `r round(min(colAUC(all_pred, ytest,)),2)` et `r round(max(colAUC(all_pred, ytest,)),2)`. Il ne s'agit donc pas d'exellents modèles.

# Réalisation du réseau de neurones

Nous allons voir le protocole expérimentale, puis nous allons le décrire plus en détail le déroulement de l'analyse réalisé ainsi que les conclusions.

## Protocole expérimental



## Transformation des données

Avant de réaliser les réseaux de neurones, nous devons transformer les données. Pour les variables quantitatives, nous allons les normaliser pour qu'elle ai la même importance. Pour les variables qualitatives il faut les convertir en ordonné en leur donnant une valeur numeric de 1 à k ou k est le nombre de facteur. 
Avant de paramètrer le réseau de neuronne, il faut normaliser les données, les variables explicatives, dans notre cas toutes les variables du jeu de données sauf "y" la variables cible.

```{r}
#on récupère les données de base
datann<-data
dataquali<-data[,c(2,3,4,5,7,8,9,11,16)]

#quali en quali numeric
#fonction qui change pour une variable les noms des modalité par un nombre :
encoding<-function(var) {
    cpt=0
    for (i in levels(as.factor(var))) {
        for (j in length(var)) {
            var[var==i]<-cpt
        }
        cpt=cpt+1

    }
    return(as.numeric(var))
}
dataquali<-lapply(dataquali, encoding)

#donnée avec toute les variables numérique :
datann[,c(2,3,4,5,7,8,9,11,16)]<-dataquali

#normalisation des variables 
scaleddata<-scale(datann[-17])
datann<-data.frame(scaleddata,datann[17])
```

De plus, pour la vérification et la validation du meilleur reseau de neurone, nous découpons nos données en trois échantillons. Nous réalisons un premier découpage dans les données pour obtenir un échantillon d'apprentissage qui est constitué de 80% de la base et un échantillon test qui est constitué de 20% de la base. Puis nous re-découpons l'échantillon d'apprentissage en deux, pour récupérer un échantillon de validation qui est constitué de 30% de l'échantillon d'apprentissage. 

```{r}
#Echantillonage des données
ech = sort(sample(nrow(datann), nrow(datann)*.8))

train = datann[ech, ]
test = datann[-ech, ]

#Definition variables explicatives/variable cible de test
Xtest = test[, -17]
ytest = test[, 17]

#Découpage de l'échantillon d'apprentissage en ech de validation : 
#Echantillonage des données
ech2 = sort(sample(nrow(train), nrow(train)*.7))

valid = train[-ech2, ]
train = train[ech2, ]

#Definition variables explicatives/variable cible 
Xtrain = train[, -17]
ytrain = train[, 17]
Xvalid = valid[, -17]
yvalid = valid[, 17]

```


## Paramétrage du réseau de neurone

Nous allons réaliser un réseau de neurones à l'aide de la fonction "neutralnet" qui permet de réaliser tout type de réseau de neurones.
Nous allons paramétrer cette méthode grâce à de nombreux paramètres : 
 - hidden : spécifie le nombre de neuronnes dans la couche caché
 - err.fct : fonction utilisé pour determiner le calcul de l'erreur (ce : entropie croisée, sse : somme des erreurs au carré)
 - linear.output = FALSE : Ne pas réaliser de regression linéaire
 - algorithm : contient une chaine définissant le type d'algorithme, par défaut vaut "rprop+" qui signifie rétropropagation résiliente avec retour de poids
 - act.fct : permet de lisser le résultat du produit croisé des neurones et des poids, par défaut "logistic" qui se rapporte à la fonction logistique.

Cette fonction utilise d'autre paramètres comme : 
 - startweights : contient la valeurs de départ des poids (par défaut une initialisation aléatoire)
 - rep : Nombre d'entrainement du réseau (pas utiliser pour éviter le sur-apprentissage)

Dans le cas d'une variable y catégorielle binaire, nous choisissons comme fonction d'erreur l'entropie croissée.

Premier modéle : 

```{r}
nn=neuralnet(ytrain~., data = Xtrain, hidden=10, err.fct="ce",linear.output = FALSE)
attributes(nn)
nn$result.matrix

```

Validation du modèle : 

```{r}
prob <- predict(nn,valid)
pred <- ifelse(prob>0.5, 1, 0)
pred

```

Voici la matrice de confusion entre les valeurs prédite et les valeurs test et le taux d'erreur :  
```{r}
m<-table(pred,yvalid)
m
#tx d'erreur :
tx<-(m[1,2]+m[2,1])/length(yvalid)
tx
```

Le taux d'erreur de ce premier modéle est de `r round(tx*100,2)`%.

Deuxième modèle : 
```{r}
nn2=neuralnet(ytrain~., data = Xtrain, hidden=10, err.fct="ce",linear.output = FALSE,likelihood = TRUE)

prob2 <- predict(nn2,valid)
pred2 <- ifelse(prob2>0.5, 1, 0)

m2<-table(pred2,yvalid)
tx2<-(m2[1,2]+m2[2,1])/length(yvalid)
tx2

```
Le modéle deux est moins performant avec le "likelihood" (taux d'erreur = `r round(tx*100,2)`% > `r round(tx2*100,2)`%).

Troixième modèle : augmentation du nombre de couche de la couche caché de 10 à 17 (nombre de variable d'entrée)
```{r}
nn3=neuralnet(ytrain~., data = Xtrain, hidden=17, err.fct="ce",linear.output = FALSE)

prob3 <- predict(nn3,valid)
pred3 <- ifelse(prob3>0.5, 1, 0)

m3<-table(pred3,yvalid)
tx3<-(m3[1,2]+m3[2,1])/length(yvalid)
tx3

```

Le taux d'erreur est plus faible que pour une couche comprenant 10 neurones cachés (`r round(tx3*100,2)`%)

Quatrième modèle : augmentation du nombre de couche de la couche caché de 17 à 34 (deux fois le nombre de variable d'entrée)
```{r}
nn4=neuralnet(ytrain~., data = Xtrain, hidden=34, err.fct="ce",linear.output = FALSE)

prob4 <- predict(nn4,valid)
pred4 <- ifelse(prob4>0.5, 1, 0)

m4<-table(pred4,yvalid)
tx4<-(m4[1,2]+m[2,1])/length(yvalid)
tx4

```

Taux d'erreur un peu élevé, donc moins intéressant (`r round(tx4*100,2)`%)

On garde la prédiction n°3 pour le nombre de neurones de la couche caché. Et nous décidons de garder 17 neurones dans la couche caché.

Cinquième modèle : On utilise une couche caché composé de 34 neurones et on passe l'argument "likelihood" à vrai.
```{r}
nn5<-neuralnet(ytrain~., data = Xtrain, hidden=34, err.fct="ce",linear.output = FALSE, likelihood = TRUE)

prob5 <- predict(nn5,valid)
pred5 <- ifelse(prob5>0.5, 1, 0)

m5<-table(pred5,yvalid)
tx5<-(m5[1,2]+m[2,1])/length(yvalid)
tx5

```

Cette fois ci le taux d'erreur est plus élevé (`r round(tx5*100,2)`%)

Nous testons un dernier modéle.

Sixième modèle : Nous augumentons encore une fois le nombre de neurones de la couche cachée (3 fois le nombre de variable d'entrée)
```{r}
nn6<-neuralnet(ytrain~., data = Xtrain, hidden=51, err.fct="ce",linear.output = FALSE, likelihood = TRUE)

prob6 <- predict(nn6,valid)
pred6 <- ifelse(prob6>0.5, 1, 0)

m6<-table(pred6,yvalid)
tx6<-(m6[1,2]+m[2,1])/length(yvalid)

```
Cette fois le taux d'erreur est de `r round(tx6*100,2)`% ce qui est faiblement plus élevé que pour le quatrième modèle (`r round(tx4*100,2)`%). Mais ce modèle utilise un plus grand nombre de neurone ce qui peut engendrer du sur-apprentissage. 

Avec la library "neutralnn" et la variable cible y correspondant à une classification binaire, il n'est pas possible de paramétrer le réseau de neurone à l'infinie. Les paramétrages pour une telle variable sont faible. Le paramètre "algorithme" ne fonctionne qu'avec "rprop +", la fonction d'erreur ne peut pas être modifier et la demande d'une régression logistique est impossible. Nous aurions pu changer les poids les variables d'entrée, mais le choix de l'aléatoire nous pariassait plus juste.
Néanmoins, avec les réseaux de neurones que nous avons construit nous avons un taux d'erreur plutôt faible, ce qui nous paraissait être un bon résultat.Pour confirmer cette avis, nous décidons de comparer les courbes ROC des six modèles.

courbe ROC : 
```{r}
all_pred = data.frame(pred, pred2,  pred4, pred5, pred6)
colnames(all_pred) = c("nn","nn2","nn4","nn5","nn6")
colAUC(all_pred, yvalid, plotROC = TRUE)
abline(0,1, col = "blue")

```

A l'aide de la Courbe ROC ainsi que du taux d'erreur, nous considérons le meilleur réseau de neurone étant le quatrième modèle construit. C'est a dire que nous utilisons 17 neuronnes caché et que nous plaçons à vrai l'argument "likelihood". C'est avec ce modèle que nous allons vérifier la bonne prédiction des valeurs test.

## Prédiction de la variable cible y

Représentation graphique du réseau de neurones séléctionné : 

```{r}
plot(nn4)

```

Voici la distribution de la variable y prédite par le réseau de neurone :

```{r}
prob <- predict(nn4,test)
predRN <- ifelse(prob>0.5, 1, 0)

```

Voici la matrice de confusion entre les valeurs prédite et les valeurs test : 
```{r}
mRN<-table(predRN,ytest)
kable(mRN)
```

#Comparaison des 3 modèles séléctionnée

Nous allons maintenant comparer les trois modèles séléctionné pour chaque méthodes. Puis nous choisirons le plus efficasse de tous.

```{r}
#tx d'erreur reseau de neurone:
txRN<-(mRN[1,2]+mRN[2,1])/length(ytest)
```
Le taux d'erreur du modèle de la regression logistique est de `r round(txLog*100,2)`%, celui du réseau de neurones est `r round(txRN*100,2)`% et enfin celui du SVM est 

courbe ROC représentant : 
```{r}
all_pred<-data.frame(predLog, predRN)
colnames(all_pred)=c("Régression logistique","Reseau de neurones")
colAUC(all_pred, ytest, plotROC = TRUE)
abline(0,1, col = "blue")

```

L'air sous la courbe représentant le reseau de neurones est supérieur à celle de la courbe représentant l'alétoire.