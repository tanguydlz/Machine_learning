#Package
```{r}
library(csv)
library(dplyr)
library(e1071)
library(caTools)
```


# Préparation des données
```{r}
#Chargement des données
data<-read.csv2("bank.csv")

#Recodage de la variable cible
data$y = recode(data$y, "no" = 0, "yes" = 1)
prop.table(table(data$y))

```
# Echantillonage
Nous séparons les données en un échantillon d'apprentissage et un échantillon de test puis nous séparons la variable cible et les variables explicatives dans des variables différentes.
```{r}
set.seed(123)
#Echantillonage des données
ech = sort(sample(nrow(data), nrow(data)*.7))
train = data[ech, ]
test = data[-ech, ]

#Definition variables explicatives/variable cible
Xtrain = train[, -17]
ytrain = train[, 17]
Xtest = test[, -17]
ytest = test[, 17]
```

# SVM
Nous utilisons le package ’e1071’ pour l’implémentation des SVM. Nous demandons à la procédure svm() de construire un classifieur dont on ne centre pas les valeurs avec un noyau de type sigmoid.
On applique cette fonction à nos données d'apprentissage
```{r}
model <- svm(model.matrix(~., Xtrain), ytrain, scale=F, type= "C-classification",kernel='sigmoid')
summary(model)
```

On obtient 692 points supports ce qui est par rapport à la taille d'échantillon (3164 observations) n'est pas forcément trop élevé, donc la modélisation est efficace


```{r}
#Prédiction sur les données test
pred = predict(model, newdata = model.matrix(~., Xtest))
#Matrice de confusion
cm = table(pred, ytest); cm
#Taux d'erreur
err = (cm[1,2] + cm[2,1])/sum(cm); err
```
Le taux d'erreur est de `r round(err*100,2)`% qui est donc moins bonne que la prédiction par défaut. (11.52%)

Ce model n'ayant pas été tuné, il n'est que la version simple de ce que nous pouvons avoir.
Maintenant, nous allons essayer d'ajuster notre modèle en tunant pour le moment deux hyperparamètres : C et Gamma.

Pour rappel, l'hyperparamètre C est responsable de la taille de la marge du MVC. Cela signifie que les points situés à l'intérieur de cette marge ne sont classés dans aucune des deux catégories.  Plus la valeur de C est faible, plus la marge est importante

L'hyperparamètre gamma doit être réglé pour mieux adapter l'hyperplan aux données.  Il est responsable du degré de linéarité de l'hyperplan, et pour cela, il n'est pas présent lors de l'utilisation de noyaux linéaires. Plus γ est petit, plus l'hyperplan aura l'air d'une ligne droite, tandis que si γ est trop grand, l'hyperplan sera plus courbé et pourrait trop bien délimiter les données, ce qui entraînerait un overfitting.


#Optimisation des hyperparamètres
```{r}
#Optimisation de Gamma et C

tuned = tune.svm(x=model.matrix(~., Xtrain),
                 y=as.factor(ytrain), 
                 scale=F, type = "C-classification", kernel='sigmoid',
                 cost = 10^(-1:2), 
                 gamma = c(0.1, 1, 10),
                 tunecontrol=tune.control(cross=10))

tuned$performances
```

Le tableau met en évidence les différentes valeurs de gamma et C testées avec le taux d'erreur moyen pour 10 validations.
On constate donc que 3 modèles sortent du lot avec des taux d'erreur presque similaire au alentour de 12.8% :
gamma : 0.1 & cost : 0.1
gamma : 1.0 & cost : 0.1
gamma : 10.0 & cost : 0.1

```{r}
plot(tuned)
```

Nous allons donc faire nos prédictions avec deux modèles, étant donné que nous avons le même taux d'erreur pour deux modèles, nous prendrons donc celui recommendé par l'algorithme.


```{r}
svmfit = tuned$best.model
#Prédiction sur les données test
pred2 = predict(svmfit, newdata = model.matrix(~., Xtest))
#Matrice de confusion
cm = table(pred2, ytest); cm
#Taux d'erreur
err = (cm[1,2] + cm[2,1])/sum(cm); err
```

```{r}
model2=svm(model.matrix(~., Xtrain), ytrain, scale=F,cost=0.1, gamma=0.1, type= "C-classification",kernel='sigmoid')
#Prédiction sur les données test
pred3 = predict(model2, newdata = model.matrix(~., Xtest))
#Matrice de confusion
cm = table(pred3, ytest); cm
#Taux d'erreur
err = (cm[1,2] + cm[2,1])/sum(cm); err
```
On constate donc que nous avons le même taux d'erreur. Cependant, nous avons une moins bonne prédiction que celle par défaut (13.19% contre 11.52%)

Cependant, nous avons essayé que peu de valeur pour nos hyperparamètres et avons obtenu de bons résultats, on va donc élargir notre champ de valeur possible pour C et Gamma, pour essayer d'obtenir de meilleur résultat tout en évitant de l'overfitting.

#Optimisation des hyperparamètres
```{r}
#Optimisation de Gamma et C

tuned2 = tune.svm(x=model.matrix(~., Xtrain),
                 y=as.factor(ytrain), 
                 scale=F, type = "C-classification", kernel='sigmoid',
                 cost = 10^(-1:3), 
                 gamma = 10^(-5:-1),
                 tunecontrol=tune.control(cross=10))

tuned2$performances
```

On a augmenté le champ de valeur possible pour C et diminué celui de gamma, car c'est ce qui correspond le mieux pour avoir le meilleur modèle possible. Au vu des différents taux d'erreur, nous n'avons pas de meilleurs résultats comparé à la précédente cross validation. On pourrait donc supposer que notre meilleur modèle pour ce jeu de données seraient avec C = 0.1 et gamma = 0.1


Nous allons maintenant afficher la courbe ROC de nos deux modèles à comparer

```{r}
pred=as.numeric(pred)
pred=recode(pred, '1'=0, '2'=1)
pred2=as.numeric(pred2)
pred2=recode(pred2, '1'=0, '2'=1)
```

```{r}
all_pred = data.frame(pred, pred2)
colnames(all_pred) = c("svm_W/O_cv", "svm_w/_cv")
colAUC(all_pred, ytest, plotROC = TRUE)
abline(0,1, col = "blue")
```
On remarque que parmi nos 2 modèles, un fait mieux que celui aléatoire et l'autre le surpasse très légèrement
Les aires sous la courbes sont compris entre `r round(min(colAUC(all_pred, ytest,)),2)` et `r round(max(colAUC(all_pred, ytest,)),2)`.
Ces deux modèles ne sont donc pas performant.

