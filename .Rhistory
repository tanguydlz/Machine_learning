library(gamlss.add)
install.packages("gamlss.add")
library(gamlss.add)
data<-read.csv2("bank.csv")
#Transformation de y en variable bianire :
data$y = recode(data$y, "no" = 0, "yes" = 1)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(glmnet)
library(caTools)
#Transformation de y en variable bianire :
data$y = recode(data$y, "no" = 0, "yes" = 1)
#Echantillonage des données
ech = sort(sample(nrow(data), nrow(data)*.7))
train = data[ech, ]
test = data[-ech, ]
#Definition variables explicatives/variable cible
Xtrain = train[, -17]
ytrain = train[, 17]
Xtest = test[, -17]
ytest = test[, 17]
summary(train)
model <- nnet(y=ytrain, x=Xtrain, size = 10,)
model <- nnet(y=ytrain, x=Xtrain, size = 10)
data$y
ytrain
Xtrain
nnet(y=ytrain, x=Xtrain, size = 10)
model <- nnet(ytrain~., data=train, size = 10)
plot(model)
plot(model,nid=F)
pred <- predict(model, newdata = test)
table(pred)
plot.nnet(model,pos.col='darkgreen',neg.col='darkblue',alpha.val=0.7,rel.rsc=15,
circle.cex=10,cex=1.4,
circle.col='brown')
plot.nnet(model,pos.col='darkgreen',neg.col='darkblue',alpha.val=0.7,rel.rsc=15,
circle.cex=10,cex=1.4,
circle.col='brown')
plot(model,pos.col='darkgreen',neg.col='darkblue',alpha.val=0.7,rel.rsc=15,
circle.cex=10,cex=1.4,
circle.col='brown')
model <- nnet(ytrain~., data=train, size = 10,decay=5e-4, maxit=200)
pred <- predict(model, newdata = test)
table(pred)
model <- nnet(ytrain~., data=train, size = 10,decay=5e-4, maxit=200, linout=FALSE)
pred <- predict(model, newdata = test)
table(pred)
m<-table(pred,test)
m<-table(pred,ytest)
m
df<-read.xlsx2("BDD_CEDA_dec_2020_complete - ANONYME.xlsx", sheetIndex = 1,header = TRUE)
library(xlsx)
df<-read.xlsx2("BDD_CEDA_dec_2020_complete - ANONYME.xlsx", sheetIndex = 1,header = TRUE)
#source
preoccupation_source=VectorSource(df$Préoccupations)
library(tm)
library(ngram)
library(wordcloud)
BigramTokenizer =function(x){
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
OnegramTokenizer =function(x){
unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}
#source
preoccupation_source=VectorSource(df$Préoccupations)
#corpus
preoccupation_corpus=VCorpus(preoccupation_source)
#TransformMinuscule
preoccupation_corpus= tm_map(preoccupation_corpus,content_transformer(tolower))
#removePunctuation
preoccupation_corpus = tm_map(preoccupation_corpus,removePunctuation)
#removeStopWord
preoccupation_corpus = tm_map(preoccupation_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
preoccupation_corpus = tm_map(preoccupation_corpus,stripWhitespace)
preoccupation_matrix=TermDocumentMatrix(preoccupation_corpus, control = list(tokenize=BigramTokenizer))
#matrice
M_preoccupation <- as.matrix(preoccupation_matrix)
print(nrow(M_preoccupation))
print(ncol(M_preoccupation))
#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_preoccupation <- apply(M_preoccupation,1,sum)
print(sort(freq_preoccupation,decreasing=TRUE)[1:10])
wordcloud(names(freq_preoccupation),freq_preoccupation,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
wordcloud(names(freq_preoccupation),freq_preoccupation,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
preoccupation_matrix2=TermDocumentMatrix(preoccupation_corpus, control = list(tokenize=OnegramTokenizer))
#matrice
M_preoccupation2 <- as.matrix(preoccupation_matrix2)
print(nrow(M_preoccupation2))
print(ncol(M_preoccupation2))
#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_preoccupation2 <- apply(M_preoccupation2,1,sum)
print(sort(freq_preoccupation2,decreasing=TRUE)[1:10])
wordcloud(names(freq_preoccupation2),freq_preoccupation2,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
wordcloud(names(freq_preoccupation2),freq_preoccupation2,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
wordcloud(names(freq_preoccupation),freq_preoccupation,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
wordcloud(names(freq_preoccupation),freq_preoccupation,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
#chargement des donnees
#setwd("C:/Users/ameli/Desktop/R/data_mining")
#le package nnet permet de réaliser dess reseau de neurones avec une seule couche caché :
library(nnet)
data<-read.csv2("bank.csv")
#Transformation de y en variable bianire :
data$y = recode(data$y, "no" = 0, "yes" = 1)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(glmnet)
library(caTools)
#Transformation de y en variable bianire :
data$y = recode(data$y, "no" = 0, "yes" = 1)
#Echantillonage des données
ech = sort(sample(nrow(data), nrow(data)*.7))
train = data[ech, ]
test = data[-ech, ]
model <- nnet(ytrain~., data=train, size = 10,decay=5e-4, maxit=200, linout=FALSE)
#Definition variables explicatives/variable cible
Xtrain = train[, -17]
train = data[ech, ]
test = data[-ech, ]
#Definition variables explicatives/variable cible
Xtrain = train[, -17]
ytrain = train[, 17]
Xtest = test[, -17]
ytest = test[, 17]
model <- nnet(ytrain~., data=train, size = 10,decay=5e-4, maxit=200, linout=FALSE)
library(neuralnet)
library(neuralnet)
n <- neuralnet(ytrain~.,
data = train,
hidden = 5,
err.fct = "ce",
linear.output = FALSE,
lifesign = 'full',
rep = 2,
algorithm = "rprop+",
stepmax = 100000)
train$age
train$balance
train$poutcome
train
n <- neuralnet(ytrain~train$age+train$balance+train$day,
data = train,
hidden = 5,
err.fct = "ce",
linear.output = FALSE,
lifesign = 'full',
rep = 2,
algorithm = "rprop+",
stepmax = 100000)
n <- neuralnet(ytrain~train$age+train$balance+train$duration,
data = train,
hidden = 5,
err.fct = "ce",
linear.output = FALSE,
lifesign = 'full',
rep = 2,
algorithm = "rprop+",
stepmax = 100000)
n <- neuralnet(ytrain~age+balance+duration,
data = train,
hidden = 5,
err.fct = "ce",
linear.output = FALSE,
lifesign = 'full',
rep = 2,
algorithm = "rprop+",
stepmax = 100000)
plot(n)
plot(n, rep=1)
nn=neuralnet(ytrain~age+balance+duration,
data = train, hidden=3,act.fct = "logistic",
linear.output = FALSE)
plot(nn)
Predict=compute(nn,test)
prob <- Predict$net.result
pred <- ifelse(prob>0.5, 1, 0)
pred
m<-table(pred,ytest)
m
tx<-(m[1,2]+m[2,1])/length(y)
m[1,2]
#tx d'erreur :
tx<-(m[1,2])/length(y)
#tx d'erreur :
tx<-(m[1,2])/length(ytest)
tx
m
data
#normalisation :
scaleddata<-scale(data$age,data$balance,data$day,data$duration,data$pdays,data$previous)
#normalisation :
scaleddata<-scale(data.frame(data$age,data$balance,data$day,data$duration,data$pdays,data$previous))
datat(boston)
data(boston)
library(MASS)
data(boston)
data("Boston")
Boston
nn=neuralnet(ytrain~age+balance+duration+job,
data = train, hidden=3,act.fct = "logistic",
linear.output = FALSE)
#on récupère les données de base
datann<-datan
#on récupère les données de base
datann<-data
datann
#quali en quali numeric
data$job
#quali en quali numeric
levels(data$job)
#quali en quali numeric
levels(as.factor(data$job))
var=data$job
levels(as.factor(data$job))[1]
i=levels(as.factor(data$job))[1]
i
var[i]
var[str(i)]
var
for (i in levels(as.factor(var))) {
var<-recode(var, str(i) = cpt)
cpt=0
for (i in levels(as.factor(var))) {
var<-recode(var, str(i) = cpt)
cpt=0
for (i in levels(as.factor(var))[-1]) {
var<-recode(var, str(i) = cpt,str(i+1)=cpt+1)
str(i)
i
for (i in levels(as.factor(var))[-1]) {
var<-recode(var, i= cpt,i+1=cpt+1)
levels(as.factor(var))
for (i in levels(as.factor(var))[-1]) {
var<-recode(var, "admin."= cpt,"blue-colla"=cpt+1)
cpt=cpt+1
}
var
var=data$job
cpt=0
for (i in levels(as.factor(var))[-1]) {
var[i==i]<-cpt
cpt=cpt+1
}
var
cpt=0
for (i in levels(as.factor(var))[-1]) {
for (j in length(var)) {
var[var[j]==i]<-cpt
}
cpt=cpt+1
}
var
var=data$job
cpt=0
for (i in levels(as.factor(var))[-1]) {
for (j in length(var)) {
var[var[j]==i]<-cpt
}
cpt=cpt+1
}
var
i
j
var[j]
var=data$job
var[j]
var[j]==i
var[var[j]==i]
var==i
var=data$job
cpt=0
for (i in levels(as.factor(var))[-1]) {
for (j in length(var)) {
var[var==i]<-cpt
}
cpt=cpt+1
}
var
cpt=0
for (i in levels(as.factor(var))[-1]) {
for (j in length(var)) {
var[var==i]<-cpt
}
cpt=cpt+1
}
cpt=0
for (i in levels(as.factor(var))) {
for (j in length(var)) {
var[var==i]<-cpt
}
cpt=cpt+1
}
var
var=data$job
cpt=0
for (i in levels(as.factor(var))) {
for (j in length(var)) {
var[var==i]<-cpt
}
cpt=cpt+1
var
}
for (i in levels(as.factor(var))) {
for (j in length(var)) {
var[var==i]<-cpt
}
cpt=cpt+1
}
var
cpt=0
var=data$job
for (i in levels(as.factor(var))) {
for (j in length(var)) {
var[var==i]<-cpt
}
cpt=cpt+1
}
var
data
as.numeric(var)
lapply(data, encoding)
encoding<-function(var) {
cpt=0
for (i in levels(as.factor(var))) {
for (j in length(var)) {
var[var==i]<-cpt
}
cpt=cpt+1
}
return(as.numeric(var))
}
lapply(data, encoding)
dataquali<-lapply(dataquali, encoding)
dataquali<-data[,c(2,3,4,5,7,8,9,11,16)]
dataquali<-lapply(dataquali, encoding)
dataquali
datann[,c(2,3,4,5,7,8,9,11,16)]<-dataquali
datann
#normalisation des variables
scaleddata<-scale(datann)
#Echantillonage des données
ech = sort(sample(nrow(scaleddata), nrow(scaleddata)*.7))
train = scaleddata[ech, ]
test = scaleddata[-ech, ]
#Definition variables explicatives/variable cible
Xtrain = scaleddata[, -17]
ytrain = scaleddata[, 17]
Xtest = scaleddata[, -17]
ytest = scaleddata[, 17]
summary(train)
train$y
train
#normalisation des variables
scaleddata<-scale(datann[-17])
scaleddata+datann[17]
datann<-data.frame(scaleddata,datann[17])
datann
#Echantillonage des données
ech = sort(sample(nrow(scaleddata), nrow(scaleddata)*.7))
train = scaleddata[ech, ]
ech = sort(sample(nrow(datann), nrow(datann)*.7))
train = datann[ech, ]
test = datann[-ech, ]
#Definition variables explicatives/variable cible
Xtrain = datann[, -17]
ytrain = datann[, 17]
Xtest = datann[, -17]
ytest = datann[, 17]
''' n <- neuralnet(ytrain~age+balance+duration,
data = train,
hidden = 5,
err.fct = "ce",
linear.output = FALSE,
lifesign = "full",
rep = 2,
algorithm = "rprop+",
stepmax = 100000)'''
# n <- neuralnet(ytrain~age+balance+duration,
#              data = train,
#             hidden = 5,
#            err.fct = "ce",
#           linear.output = FALSE,
#          lifesign = "full",
#         rep = 2,
#        algorithm = "rprop+",
#       stepmax = 100000)'''
nn=neuralnet(ytrain~age+balance+duration+job,
data = train, hidden=3,act.fct = "logistic",
linear.output = FALSE)
nn=neuralnet(ytrain~.,
data = train, hidden=10,act.fct = "logistic",
linear.output = FALSE)
