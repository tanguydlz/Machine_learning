% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data\_mining},
  pdfauthor={Axelle Barou \& Amélie Picard \& Tanguy Delzant},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Data\_mining}
\author{Axelle Barou \& Amélie Picard \& Tanguy Delzant}
\date{06/01/2021}

\begin{document}
\maketitle

Rapport

\#Présentation de la problématique et du jeu de données

Nous avons choisi un jeu de données sur Kaggle :
\url{https://www.kaggle.com/volodymyrgavrysh/bank-marketing-campaigns-dataset}

Ce dernier contient 41188 observations et 21 variables.

Il s'agit d'un jeu de données sur les campagnes marketing des banques
portugaises. Ces campagnes, qui se basaient sur des appels
téléphoniques, permettaient d'offrir aux clients de déposer un dépôt à
terme, c'est-à-dire de prêter de l'argent à une banque sur une durée
fixe, avec un taux d'intérêt connu à l'avance ou variable.

Notre problématique sera donc d'essayer de prédire, en fonction des
données du client, des données récoltées au cours de l'entretien
téléphonique de la campagne et d'autres attributs sociaux et économiques
si ces clients seront enclins à souscrire un dépôt à terme.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#chargement des donnees}
\CommentTok{#setwd("C:/Users/ameli/Desktop/R/data_mining")}

\CommentTok{#le package neutralnet permet de réaliser des reseau de neurones :}
\KeywordTok{library}\NormalTok{(neuralnet) }
\KeywordTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:neuralnet':
## 
##     compute
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     filter, lag
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: Matrix
\end{verbatim}

\begin{verbatim}
## Loaded glmnet 4.0-2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caTools)}

\NormalTok{data<-}\KeywordTok{read.csv2}\NormalTok{(}\StringTok{"bank.csv"}\NormalTok{)}

\CommentTok{#Transformation de y en variable bianire : }
\NormalTok{data}\OperatorTok{$}\NormalTok{y =}\StringTok{ }\KeywordTok{recode}\NormalTok{(data}\OperatorTok{$}\NormalTok{y, }\StringTok{"no"}\NormalTok{ =}\StringTok{ }\DecValTok{0}\NormalTok{, }\StringTok{"yes"}\NormalTok{ =}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\#Réalisation du réseau de neurones

Nous allons voir le protocole expérimentale, puis nous allons le décrire
plus en détail le déroulement de l'analyse réalisé ainsi que les
conclusions.

\#\#Protocole expérimental

\#\#Transformation des données

Avant de réaliser les réseaux de neurones, nous devons transformer les
données. Pour les variables quantitatives, nous allons les normaliser
pour qu'elle ai la même importance. Pour les variables qualitatives il
faut les convertir en ordonné en leur donnant une valeur numeric de 1 à
k ou k est le nombre de facteur. Avant de paramètrer le réseau de
neuronne, il faut normaliser les données, les variables explicatives,
dans notre cas toutes les variables du jeu de données sauf ``y'' la
variables cible.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#on récupère les données de base}
\NormalTok{datann<-data}
\NormalTok{dataquali<-data[,}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{16}\NormalTok{)]}

\CommentTok{#quali en quali numeric}
\CommentTok{#fonction qui change pour une variable les noms des modalité par un nombre :}
\NormalTok{encoding<-}\ControlFlowTok{function}\NormalTok{(var) \{}
\NormalTok{    cpt=}\DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{levels}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(var))) \{}
        \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \KeywordTok{length}\NormalTok{(var)) \{}
\NormalTok{            var[var}\OperatorTok{==}\NormalTok{i]<-cpt}
\NormalTok{        \}}
\NormalTok{        cpt=cpt}\OperatorTok{+}\DecValTok{1}

\NormalTok{    \}}
    \KeywordTok{return}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(var))}
\NormalTok{\}}
\NormalTok{dataquali<-}\KeywordTok{lapply}\NormalTok{(dataquali, encoding)}

\CommentTok{#donnée avec toute les variables numérique :}
\NormalTok{datann[,}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{7}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{9}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{16}\NormalTok{)]<-dataquali}

\CommentTok{#normalisation des variables }
\NormalTok{scaleddata<-}\KeywordTok{scale}\NormalTok{(datann[}\OperatorTok{-}\DecValTok{17}\NormalTok{])}
\NormalTok{datann<-}\KeywordTok{data.frame}\NormalTok{(scaleddata,datann[}\DecValTok{17}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

De plus, pour la vérification et la validation du meilleur reseau de
neurone, nous découpons nos données en trois échantillons. Nous
réalisons un premier découpage dans les données pour obtenir un
échantillon d'apprentissage qui est constitué de 80\% de la base et un
échantillon test qui est constitué de 20\% de la base. Puis nous
re-découpons l'échantillon d'apprentissage en deux, pour récupérer un
échantillon de validation qui est constitué de 30\% de l'échantillon
d'apprentissage.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Echantillonage des données}
\NormalTok{ech =}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(datann), }\KeywordTok{nrow}\NormalTok{(datann)}\OperatorTok{*}\NormalTok{.}\DecValTok{8}\NormalTok{))}

\NormalTok{train =}\StringTok{ }\NormalTok{datann[ech, ]}
\NormalTok{test =}\StringTok{ }\NormalTok{datann[}\OperatorTok{-}\NormalTok{ech, ]}

\CommentTok{#Definition variables explicatives/variable cible de test}
\NormalTok{Xtest =}\StringTok{ }\NormalTok{test[, }\DecValTok{-17}\NormalTok{]}
\NormalTok{ytest =}\StringTok{ }\NormalTok{test[, }\DecValTok{17}\NormalTok{]}

\CommentTok{#Découpage de l'échantillon d'apprentissage en ech de validation : }
\CommentTok{#Echantillonage des données}
\NormalTok{ech2 =}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(train), }\KeywordTok{nrow}\NormalTok{(train)}\OperatorTok{*}\NormalTok{.}\DecValTok{7}\NormalTok{))}

\NormalTok{valid =}\StringTok{ }\NormalTok{train[}\OperatorTok{-}\NormalTok{ech2, ]}
\NormalTok{train =}\StringTok{ }\NormalTok{train[ech2, ]}

\CommentTok{#Definition variables explicatives/variable cible }
\NormalTok{Xtrain =}\StringTok{ }\NormalTok{train[, }\DecValTok{-17}\NormalTok{]}
\NormalTok{ytrain =}\StringTok{ }\NormalTok{train[, }\DecValTok{17}\NormalTok{]}
\NormalTok{Xvalid =}\StringTok{ }\NormalTok{valid[, }\DecValTok{-17}\NormalTok{]}
\NormalTok{yvalid =}\StringTok{ }\NormalTok{valid[, }\DecValTok{17}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\#\#Paramétrage du réseau de neurone

Nous allons réaliser un réseau de neurones à l'aide de la fonction
``neutralnet'' qui permet de réaliser tout type de réseau de neurones.
Nous allons paramétrer cette méthode grâce à de nombreux paramètres : -
hidden : spécifie le nombre de neuronnes dans la couche caché - err.fct
: fonction utilisé pour determiner le calcul de l'erreur (ce : entropie
croisée, sse : somme des erreurs au carré) - linear.output = FALSE : Ne
pas réaliser de regression linéaire - algorithm : contient une chaine
définissant le type d'algorithme, par défaut vaut ``rprop+'' qui
signifie rétropropagation résiliente avec retour de poids - act.fct :
permet de lisser le résultat du produit croisé des neurones et des
poids, par défaut ``logistic'' qui se rapporte à la fonction logistique.

Cette fonction utilise d'autre paramètres comme : - startweights :
contient la valeurs de départ des poids (par défaut une initialisation
aléatoire) - rep : Nombre d'entrainement du réseau (pas utiliser pour
éviter le sur-apprentissage)

Dans le cas d'une variable y catégorielle binaire, nous choisissons
comme fonction d'erreur l'entropie croissée.

Premier modéle :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn=}\KeywordTok{neuralnet}\NormalTok{(ytrain}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Xtrain, }\DataTypeTok{hidden=}\DecValTok{10}\NormalTok{, }\DataTypeTok{err.fct=}\StringTok{"ce"}\NormalTok{,}\DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{attributes}\NormalTok{(nn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $names
##  [1] "call"                "response"            "covariate"          
##  [4] "model.list"          "err.fct"             "act.fct"            
##  [7] "linear.output"       "data"                "exclude"            
## [10] "net.result"          "weights"             "generalized.weights"
## [13] "startweights"        "result.matrix"      
## 
## $class
## [1] "nn"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn}\OperatorTok{$}\NormalTok{result.matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                 [,1]
## error                   3.480378e+02
## reached.threshold       9.847026e-03
## steps                   1.732700e+04
## Intercept.to.1layhid1   7.269440e+00
## age.to.1layhid1         1.037945e+01
## job.to.1layhid1        -1.139462e+02
## marital.to.1layhid1    -5.563025e+01
## education.to.1layhid1   7.880516e-01
## default.to.1layhid1     1.903656e+01
## balance.to.1layhid1    -2.249269e+01
## housing.to.1layhid1     1.295328e+01
## loan.to.1layhid1       -3.195259e+01
## contact.to.1layhid1     4.577954e+01
## day.to.1layhid1         5.987258e+01
## month.to.1layhid1      -2.895166e+01
## duration.to.1layhid1    9.969992e+01
## campaign.to.1layhid1   -2.311982e+01
## pdays.to.1layhid1       2.733885e+01
## previous.to.1layhid1    9.623231e+01
## poutcome.to.1layhid1    3.513931e+01
## Intercept.to.1layhid2   2.798730e+01
## age.to.1layhid2         5.464410e+01
## job.to.1layhid2         2.731981e+01
## marital.to.1layhid2     6.068259e+00
## education.to.1layhid2   2.592165e+01
## default.to.1layhid2     2.321248e-01
## balance.to.1layhid2     4.278258e+01
## housing.to.1layhid2     1.207749e+00
## loan.to.1layhid2        1.084569e+01
## contact.to.1layhid2     2.115512e+00
## day.to.1layhid2         2.051630e+00
## month.to.1layhid2      -9.770460e+00
## duration.to.1layhid2   -5.355232e+01
## campaign.to.1layhid2    4.872752e+00
## pdays.to.1layhid2       7.377042e+01
## previous.to.1layhid2   -3.089820e+01
## poutcome.to.1layhid2    4.028677e+01
## Intercept.to.1layhid3  -2.812357e+00
## age.to.1layhid3        -9.681019e-01
## job.to.1layhid3         6.432648e+01
## marital.to.1layhid3    -3.241194e+01
## education.to.1layhid3   4.625600e+01
## default.to.1layhid3    -2.279758e+01
## balance.to.1layhid3    -1.307053e+01
## housing.to.1layhid3    -6.870448e+00
## loan.to.1layhid3        1.865189e+01
## contact.to.1layhid3    -1.307017e+02
## day.to.1layhid3         1.554465e+01
## month.to.1layhid3       8.810099e+00
## duration.to.1layhid3    3.154282e+01
## campaign.to.1layhid3   -6.447224e+00
## pdays.to.1layhid3      -6.951099e+01
## previous.to.1layhid3   -6.225526e+00
## poutcome.to.1layhid3    3.367578e+01
## Intercept.to.1layhid4  -6.870428e+01
## age.to.1layhid4         3.687322e+00
## job.to.1layhid4        -4.980254e+01
## marital.to.1layhid4     9.769293e+01
## education.to.1layhid4  -7.929343e+01
## default.to.1layhid4     5.638603e-01
## balance.to.1layhid4    -9.536442e+00
## housing.to.1layhid4    -4.805625e-01
## loan.to.1layhid4        2.816119e+00
## contact.to.1layhid4     1.078054e+01
## day.to.1layhid4        -4.890779e+01
## month.to.1layhid4      -4.453164e+01
## duration.to.1layhid4   -1.089686e+02
## campaign.to.1layhid4    7.670988e+01
## pdays.to.1layhid4       8.748529e+00
## previous.to.1layhid4   -3.408931e+01
## poutcome.to.1layhid4   -1.480708e+01
## Intercept.to.1layhid5  -2.650929e+01
## age.to.1layhid5         7.881021e+01
## job.to.1layhid5        -5.625572e+01
## marital.to.1layhid5     9.860695e+01
## education.to.1layhid5  -4.975091e+01
## default.to.1layhid5     1.937965e+01
## balance.to.1layhid5    -1.726268e+01
## housing.to.1layhid5    -6.225164e+01
## loan.to.1layhid5       -4.416378e+01
## contact.to.1layhid5    -1.226815e+02
## day.to.1layhid5        -9.493394e+00
## month.to.1layhid5       3.786219e+01
## duration.to.1layhid5    4.362083e+01
## campaign.to.1layhid5    3.357615e+01
## pdays.to.1layhid5      -6.094734e+00
## previous.to.1layhid5    2.898300e+01
## poutcome.to.1layhid5    1.277685e+01
## Intercept.to.1layhid6   2.693271e+01
## age.to.1layhid6        -2.237274e+01
## job.to.1layhid6        -1.061381e+01
## marital.to.1layhid6     4.670909e+01
## education.to.1layhid6  -1.916968e+01
## default.to.1layhid6     1.798848e+01
## balance.to.1layhid6    -1.110600e+01
## housing.to.1layhid6    -1.729987e+01
## loan.to.1layhid6        5.057473e+01
## contact.to.1layhid6     4.321545e+01
## day.to.1layhid6        -3.590705e+01
## month.to.1layhid6      -1.051732e+01
## duration.to.1layhid6   -3.482816e+01
## campaign.to.1layhid6    3.054318e+00
## pdays.to.1layhid6      -1.983296e+02
## previous.to.1layhid6    1.136767e+02
## poutcome.to.1layhid6    8.365642e+01
## Intercept.to.1layhid7   1.205682e+01
## age.to.1layhid7        -6.654576e+01
## job.to.1layhid7        -1.864638e+01
## marital.to.1layhid7    -1.374087e+01
## education.to.1layhid7  -4.928262e+00
## default.to.1layhid7     7.408120e+01
## balance.to.1layhid7     5.283263e+01
## housing.to.1layhid7    -5.474623e+01
## loan.to.1layhid7       -3.935384e+01
## contact.to.1layhid7     6.295716e+01
## day.to.1layhid7         5.440396e+00
## month.to.1layhid7      -2.284311e+01
## duration.to.1layhid7    6.869583e+01
## campaign.to.1layhid7   -1.729257e+00
## pdays.to.1layhid7      -2.489984e+01
## previous.to.1layhid7   -2.366614e+02
## poutcome.to.1layhid7    5.005479e+01
## Intercept.to.1layhid8   2.721611e+00
## age.to.1layhid8         1.842678e+01
## job.to.1layhid8         1.182838e+01
## marital.to.1layhid8     1.927689e+01
## education.to.1layhid8   1.802989e+01
## default.to.1layhid8     5.413330e+01
## balance.to.1layhid8     7.171174e+00
## housing.to.1layhid8    -2.073104e+01
## loan.to.1layhid8       -5.339940e+01
## contact.to.1layhid8     8.061582e+01
## day.to.1layhid8        -4.447477e+01
## month.to.1layhid8      -2.396731e+01
## duration.to.1layhid8   -4.974205e+01
## campaign.to.1layhid8   -5.719882e+01
## pdays.to.1layhid8       5.544755e+01
## previous.to.1layhid8   -1.414543e+01
## poutcome.to.1layhid8    5.549970e+01
## Intercept.to.1layhid9  -9.366455e+00
## age.to.1layhid9         4.525988e+01
## job.to.1layhid9         1.829247e+01
## marital.to.1layhid9    -2.907886e+01
## education.to.1layhid9   5.769968e+00
## default.to.1layhid9     2.148331e+01
## balance.to.1layhid9    -4.615109e+01
## housing.to.1layhid9    -6.429147e+01
## loan.to.1layhid9       -1.258229e+01
## contact.to.1layhid9     2.410430e+01
## day.to.1layhid9        -6.901601e+01
## month.to.1layhid9      -9.801897e+01
## duration.to.1layhid9    1.455502e+02
## campaign.to.1layhid9   -5.245611e-01
## pdays.to.1layhid9      -3.792665e+00
## previous.to.1layhid9    9.627794e+01
## poutcome.to.1layhid9    5.965279e+01
## Intercept.to.1layhid10 -3.536726e+01
## age.to.1layhid10        5.382762e+00
## job.to.1layhid10       -2.009169e+01
## marital.to.1layhid10   -4.539573e+01
## education.to.1layhid10  2.423815e+01
## default.to.1layhid10   -3.885545e+00
## balance.to.1layhid10   -1.445302e+02
## housing.to.1layhid10    2.425226e+01
## loan.to.1layhid10      -9.828819e+00
## contact.to.1layhid10    4.060973e+00
## day.to.1layhid10        6.568301e+00
## month.to.1layhid10     -2.533050e+01
## duration.to.1layhid10  -5.358219e+01
## campaign.to.1layhid10  -3.125675e+01
## pdays.to.1layhid10     -3.487618e+01
## previous.to.1layhid10   4.590229e+01
## poutcome.to.1layhid10  -1.616825e+01
## Intercept.to.ytrain    -3.094294e+00
## 1layhid1.to.ytrain      2.311376e+00
## 1layhid2.to.ytrain     -3.579255e+00
## 1layhid3.to.ytrain      3.039913e+00
## 1layhid4.to.ytrain     -3.242888e+00
## 1layhid5.to.ytrain      1.945145e+00
## 1layhid6.to.ytrain     -1.784245e+00
## 1layhid7.to.ytrain     -3.546988e+00
## 1layhid8.to.ytrain      3.881152e+00
## 1layhid9.to.ytrain      1.675720e+00
## 1layhid10.to.ytrain    -3.126349e+00
\end{verbatim}

Validation du modèle :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nn,valid)}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(prob}\OperatorTok{>}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{pred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## 4       0
## 6       0
## 9       0
## 14      0
## 15      0
## 18      0
## 19      0
## 20      0
## 22      0
## 29      0
## 43      0
## 48      0
## 53      0
## 54      0
## 57      0
## 63      0
## 67      0
## 69      0
## 71      1
## 74      0
## 78      0
## 82      0
## 85      0
## 87      0
## 90      0
## 91      0
## 94      0
## 95      0
## 96      0
## 97      0
## 101     0
## 107     0
## 109     0
## 118     0
## 123     0
## 124     1
## 125     1
## 130     1
## 132     0
## 144     0
## 146     0
## 147     0
## 149     0
## 151     0
## 156     0
## 158     0
## 162     0
## 168     0
## 172     0
## 177     0
## 183     0
## 185     0
## 188     0
## 192     1
## 195     0
## 198     0
## 201     1
## 204     1
## 211     0
## 215     0
## 219     0
## 220     0
## 222     0
## 227     0
## 228     0
## 230     0
## 232     0
## 237     0
## 243     0
## 245     0
## 248     0
## 250     0
## 252     0
## 260     1
## 265     0
## 274     1
## 282     0
## 291     0
## 295     0
## 299     1
## 305     0
## 309     0
## 313     1
## 321     0
## 328     1
## 332     0
## 339     0
## 342     0
## 344     0
## 347     0
## 348     0
## 351     0
## 352     0
## 353     0
## 357     0
## 360     0
## 369     0
## 371     0
## 374     0
## 376     0
## 379     0
## 381     0
## 393     1
## 398     0
## 409     0
## 410     0
## 416     1
## 421     0
## 422     0
## 423     0
## 424     0
## 426     0
## 429     0
## 431     0
## 436     0
## 439     0
## 446     0
## 448     0
## 452     0
## 462     0
## 463     0
## 465     0
## 475     0
## 479     0
## 485     0
## 492     0
## 494     0
## 499     0
## 504     0
## 508     0
## 509     0
## 511     0
## 512     1
## 517     0
## 528     0
## 533     0
## 534     1
## 541     0
## 547     0
## 551     0
## 557     0
## 558     0
## 562     0
## 563     0
## 564     0
## 574     0
## 576     0
## 580     0
## 583     0
## 584     1
## 587     0
## 589     0
## 592     0
## 595     0
## 596     0
## 597     0
## 602     0
## 604     0
## 606     0
## 609     0
## 610     0
## 616     0
## 622     0
## 624     0
## 632     0
## 638     0
## 639     0
## 643     0
## 645     0
## 651     0
## 662     1
## 664     1
## 670     0
## 672     0
## 673     0
## 674     0
## 677     0
## 682     0
## 689     0
## 690     0
## 693     0
## 696     0
## 700     0
## 706     1
## 708     0
## 715     0
## 721     0
## 722     0
## 724     0
## 726     0
## 727     0
## 735     0
## 736     0
## 737     0
## 741     1
## 744     0
## 750     0
## 756     0
## 759     0
## 761     0
## 762     0
## 765     0
## 770     0
## 774     0
## 775     0
## 778     0
## 781     0
## 782     0
## 783     0
## 784     0
## 792     0
## 800     0
## 801     1
## 804     1
## 807     0
## 812     0
## 816     0
## 818     0
## 820     0
## 824     1
## 825     0
## 826     0
## 827     0
## 828     0
## 830     0
## 836     0
## 837     0
## 838     0
## 839     0
## 842     0
## 846     0
## 849     0
## 850     0
## 858     0
## 878     0
## 881     0
## 890     0
## 895     0
## 897     0
## 901     0
## 905     0
## 912     0
## 914     0
## 917     0
## 918     0
## 920     0
## 924     0
## 926     0
## 927     0
## 928     0
## 934     0
## 943     0
## 949     0
## 953     0
## 958     0
## 966     0
## 972     0
## 973     0
## 975     0
## 977     0
## 980     0
## 985     1
## 996     0
## 1001    0
## 1004    0
## 1011    0
## 1020    0
## 1022    0
## 1026    0
## 1038    0
## 1041    0
## 1046    0
## 1052    0
## 1059    0
## 1068    0
## 1069    0
## 1072    0
## 1073    0
## 1092    0
## 1106    0
## 1112    1
## 1115    0
## 1118    0
## 1127    0
## 1136    0
## 1137    0
## 1138    0
## 1140    0
## 1144    0
## 1146    0
## 1148    0
## 1154    0
## 1158    0
## 1181    0
## 1187    0
## 1189    0
## 1191    0
## 1193    1
## 1194    0
## 1195    0
## 1208    0
## 1213    0
## 1216    0
## 1217    0
## 1222    0
## 1228    0
## 1236    0
## 1237    0
## 1238    0
## 1244    0
## 1249    0
## 1254    0
## 1255    0
## 1261    0
## 1265    0
## 1270    0
## 1272    0
## 1277    0
## 1281    0
## 1283    0
## 1288    0
## 1290    0
## 1297    1
## 1302    0
## 1303    0
## 1307    0
## 1310    0
## 1315    0
## 1317    0
## 1320    1
## 1321    1
## 1322    0
## 1323    0
## 1324    0
## 1325    0
## 1330    0
## 1336    0
## 1341    0
## 1347    0
## 1348    0
## 1350    0
## 1358    0
## 1379    0
## 1381    0
## 1385    0
## 1388    0
## 1391    0
## 1395    0
## 1404    1
## 1406    0
## 1407    0
## 1410    0
## 1411    0
## 1415    0
## 1422    0
## 1425    0
## 1428    0
## 1432    0
## 1439    0
## 1440    0
## 1441    1
## 1444    0
## 1446    0
## 1452    0
## 1464    0
## 1465    0
## 1466    0
## 1467    0
## 1475    1
## 1476    0
## 1480    0
## 1483    0
## 1487    0
## 1488    0
## 1489    0
## 1496    0
## 1498    0
## 1499    0
## 1501    0
## 1508    0
## 1514    0
## 1517    0
## 1518    0
## 1520    0
## 1525    0
## 1529    0
## 1534    0
## 1536    0
## 1550    0
## 1554    1
## 1558    0
## 1560    0
## 1578    0
## 1580    1
## 1581    0
## 1582    0
## 1587    0
## 1589    0
## 1591    0
## 1594    1
## 1598    0
## 1619    0
## 1620    0
## 1621    0
## 1628    0
## 1639    1
## 1643    0
## 1647    0
## 1650    0
## 1671    0
## 1676    0
## 1678    0
## 1682    1
## 1687    0
## 1694    0
## 1701    1
## 1702    0
## 1705    0
## 1709    0
## 1713    0
## 1714    0
## 1736    0
## 1741    0
## 1742    0
## 1743    0
## 1745    0
## 1751    1
## 1766    0
## 1769    0
## 1773    0
## 1774    0
## 1779    0
## 1781    0
## 1794    0
## 1795    0
## 1796    0
## 1799    0
## 1807    0
## 1811    0
## 1812    0
## 1813    0
## 1815    0
## 1818    0
## 1824    0
## 1829    0
## 1833    0
## 1835    0
## 1837    0
## 1852    0
## 1853    0
## 1854    0
## 1855    0
## 1859    0
## 1867    1
## 1874    0
## 1878    0
## 1883    0
## 1886    0
## 1887    1
## 1891    0
## 1893    0
## 1896    0
## 1897    1
## 1902    0
## 1904    1
## 1906    1
## 1907    0
## 1908    0
## 1909    0
## 1915    1
## 1929    0
## 1932    0
## 1933    0
## 1934    0
## 1937    0
## 1938    0
## 1947    1
## 1950    0
## 1954    0
## 1960    0
## 1961    0
## 1964    0
## 1967    0
## 1970    0
## 1972    0
## 1973    0
## 1974    0
## 1976    0
## 1981    1
## 1987    0
## 1998    0
## 2000    0
## 2004    0
## 2016    0
## 2020    0
## 2029    0
## 2030    0
## 2032    0
## 2034    1
## 2038    1
## 2041    0
## 2052    0
## 2054    0
## 2055    0
## 2056    0
## 2062    0
## 2063    0
## 2064    0
## 2070    0
## 2073    0
## 2075    0
## 2079    0
## 2083    0
## 2084    0
## 2089    0
## 2095    0
## 2097    0
## 2102    0
## 2110    0
## 2116    0
## 2118    0
## 2121    1
## 2122    0
## 2128    0
## 2130    0
## 2137    0
## 2145    1
## 2146    0
## 2158    0
## 2166    0
## 2167    0
## 2168    0
## 2170    0
## 2173    0
## 2179    0
## 2185    0
## 2187    0
## 2188    0
## 2196    0
## 2206    0
## 2207    0
## 2208    0
## 2209    0
## 2215    0
## 2216    0
## 2218    1
## 2220    0
## 2221    0
## 2223    0
## 2231    0
## 2234    0
## 2235    0
## 2248    0
## 2249    0
## 2250    0
## 2264    0
## 2266    0
## 2274    0
## 2280    0
## 2282    0
## 2283    0
## 2285    0
## 2292    0
## 2293    0
## 2306    0
## 2309    0
## 2312    0
## 2317    0
## 2336    0
## 2338    0
## 2341    0
## 2344    0
## 2349    0
## 2351    0
## 2352    0
## 2364    0
## 2365    0
## 2375    0
## 2379    0
## 2380    0
## 2383    0
## 2394    0
## 2403    0
## 2405    0
## 2406    0
## 2410    0
## 2414    0
## 2415    0
## 2418    0
## 2422    0
## 2429    0
## 2430    0
## 2440    0
## 2443    0
## 2452    0
## 2453    0
## 2460    0
## 2461    0
## 2464    0
## 2475    0
## 2478    1
## 2481    0
## 2484    0
## 2485    0
## 2490    0
## 2493    0
## 2500    0
## 2501    0
## 2503    0
## 2510    0
## 2519    0
## 2524    0
## 2525    0
## 2526    1
## 2528    0
## 2550    0
## 2553    0
## 2557    0
## 2566    0
## 2568    0
## 2570    0
## 2581    0
## 2582    0
## 2593    0
## 2595    0
## 2600    0
## 2604    1
## 2605    0
## 2608    1
## 2610    0
## 2611    0
## 2612    0
## 2613    0
## 2616    0
## 2617    0
## 2619    0
## 2622    0
## 2628    0
## 2629    0
## 2632    0
## 2634    1
## 2638    0
## 2642    1
## 2649    0
## 2653    0
## 2663    0
## 2667    0
## 2672    0
## 2676    1
## 2678    0
## 2685    0
## 2690    0
## 2696    0
## 2699    0
## 2700    0
## 2702    0
## 2705    0
## 2717    0
## 2721    0
## 2723    0
## 2730    0
## 2731    0
## 2739    0
## 2742    0
## 2743    0
## 2755    0
## 2762    0
## 2766    0
## 2769    0
## 2770    0
## 2773    1
## 2779    0
## 2786    0
## 2796    0
## 2799    0
## 2812    0
## 2813    0
## 2816    0
## 2819    1
## 2820    1
## 2822    0
## 2823    0
## 2833    0
## 2834    0
## 2842    0
## 2849    0
## 2850    0
## 2856    0
## 2859    0
## 2866    0
## 2875    0
## 2876    1
## 2877    0
## 2887    0
## 2889    0
## 2890    1
## 2894    0
## 2899    0
## 2902    0
## 2908    0
## 2909    0
## 2913    0
## 2914    0
## 2916    0
## 2917    0
## 2922    0
## 2927    0
## 2940    0
## 2948    0
## 2949    0
## 2951    0
## 2968    0
## 2969    0
## 2982    0
## 2983    1
## 2984    0
## 2990    0
## 2997    0
## 3000    0
## 3002    0
## 3004    0
## 3006    0
## 3007    0
## 3011    0
## 3025    0
## 3026    0
## 3030    0
## 3038    0
## 3045    1
## 3050    0
## 3061    0
## 3064    0
## 3066    0
## 3068    0
## 3085    0
## 3089    1
## 3090    0
## 3092    0
## 3095    1
## 3098    0
## 3102    0
## 3112    0
## 3115    0
## 3118    0
## 3121    0
## 3124    1
## 3127    0
## 3129    0
## 3131    0
## 3132    0
## 3138    1
## 3145    0
## 3152    0
## 3155    0
## 3159    0
## 3164    0
## 3166    0
## 3178    0
## 3182    0
## 3198    0
## 3204    0
## 3205    0
## 3207    0
## 3217    0
## 3218    0
## 3222    0
## 3223    0
## 3224    0
## 3241    0
## 3243    0
## 3244    1
## 3249    0
## 3255    0
## 3260    0
## 3261    0
## 3264    0
## 3269    0
## 3271    0
## 3274    0
## 3279    0
## 3281    0
## 3283    0
## 3291    0
## 3300    0
## 3305    0
## 3313    0
## 3315    1
## 3320    0
## 3322    1
## 3327    0
## 3329    0
## 3332    1
## 3338    0
## 3341    0
## 3347    0
## 3352    0
## 3353    0
## 3359    0
## 3362    0
## 3366    0
## 3371    1
## 3373    0
## 3376    0
## 3378    0
## 3382    0
## 3384    0
## 3387    0
## 3388    0
## 3392    0
## 3394    0
## 3395    0
## 3408    0
## 3413    0
## 3418    0
## 3420    1
## 3432    1
## 3434    0
## 3441    0
## 3445    0
## 3452    0
## 3455    0
## 3456    0
## 3462    0
## 3466    0
## 3467    0
## 3473    0
## 3477    0
## 3483    0
## 3487    0
## 3489    0
## 3494    0
## 3502    0
## 3504    0
## 3505    0
## 3508    0
## 3511    0
## 3513    0
## 3518    0
## 3524    0
## 3527    0
## 3531    0
## 3534    0
## 3536    0
## 3544    0
## 3549    0
## 3559    0
## 3562    0
## 3568    0
## 3573    0
## 3574    0
## 3579    0
## 3581    0
## 3583    0
## 3588    0
## 3591    0
## 3592    0
## 3593    0
## 3594    0
## 3600    0
## 3602    0
## 3603    0
## 3608    1
## 3610    0
## 3611    0
## 3614    0
## 3620    0
## 3621    0
## 3626    0
## 3631    0
## 3632    0
## 3639    0
## 3641    0
## 3644    0
## 3649    0
## 3650    0
## 3653    0
## 3660    0
## 3668    1
## 3669    0
## 3681    0
## 3691    0
## 3693    0
## 3695    0
## 3697    0
## 3700    1
## 3701    0
## 3706    0
## 3707    0
## 3734    0
## 3735    1
## 3737    0
## 3739    0
## 3741    0
## 3743    0
## 3750    0
## 3753    0
## 3758    0
## 3759    0
## 3762    0
## 3765    0
## 3767    0
## 3772    0
## 3776    0
## 3779    0
## 3788    0
## 3794    1
## 3796    0
## 3798    0
## 3801    0
## 3805    0
## 3808    0
## 3824    0
## 3826    0
## 3828    0
## 3830    0
## 3834    0
## 3836    0
## 3843    0
## 3852    0
## 3853    1
## 3855    0
## 3856    0
## 3858    0
## 3859    0
## 3865    0
## 3866    0
## 3874    1
## 3878    0
## 3887    0
## 3890    0
## 3892    0
## 3898    0
## 3900    0
## 3903    0
## 3909    0
## 3917    0
## 3921    0
## 3928    0
## 3930    1
## 3933    1
## 3941    0
## 3944    0
## 3953    0
## 3955    0
## 3958    0
## 3962    0
## 3970    0
## 3979    1
## 3986    0
## 3990    0
## 3992    0
## 3994    0
## 3998    0
## 3999    0
## 4005    0
## 4010    1
## 4012    0
## 4014    1
## 4015    0
## 4018    1
## 4024    0
## 4025    0
## 4028    0
## 4038    0
## 4040    0
## 4044    0
## 4047    0
## 4048    0
## 4052    0
## 4057    0
## 4063    0
## 4066    0
## 4075    0
## 4077    0
## 4081    0
## 4088    0
## 4089    0
## 4093    0
## 4098    0
## 4102    0
## 4109    1
## 4110    0
## 4115    0
## 4116    0
## 4136    0
## 4137    0
## 4139    0
## 4141    0
## 4142    0
## 4145    0
## 4161    1
## 4163    0
## 4164    0
## 4165    0
## 4168    0
## 4175    0
## 4178    0
## 4184    0
## 4187    0
## 4191    0
## 4200    0
## 4203    0
## 4206    0
## 4211    0
## 4216    1
## 4219    0
## 4225    1
## 4230    1
## 4232    1
## 4236    0
## 4240    0
## 4242    0
## 4244    0
## 4248    0
## 4250    0
## 4254    0
## 4260    0
## 4264    1
## 4265    0
## 4269    0
## 4271    0
## 4276    1
## 4288    0
## 4292    0
## 4305    0
## 4312    0
## 4314    0
## 4316    0
## 4317    0
## 4322    0
## 4324    0
## 4327    1
## 4328    0
## 4329    0
## 4332    0
## 4338    0
## 4339    0
## 4341    1
## 4345    0
## 4348    0
## 4352    0
## 4355    0
## 4358    0
## 4359    0
## 4360    0
## 4365    1
## 4378    0
## 4379    0
## 4384    0
## 4387    0
## 4389    0
## 4394    0
## 4398    0
## 4412    0
## 4413    0
## 4420    0
## 4425    0
## 4426    0
## 4428    0
## 4430    0
## 4432    0
## 4434    0
## 4439    0
## 4440    0
## 4441    0
## 4442    0
## 4446    0
## 4454    1
## 4456    0
## 4460    0
## 4468    0
## 4469    0
## 4473    0
## 4475    0
## 4479    0
## 4482    1
## 4495    0
## 4500    1
## 4501    0
## 4502    0
## 4503    0
## 4509    0
## 4511    0
## 4517    0
## 4518    0
\end{verbatim}

Voici la matrice de confusion entre les valeurs prédite et les valeurs
test et le taux d'erreur :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m<-}\KeywordTok{table}\NormalTok{(pred,yvalid)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     yvalid
## pred   0   1
##    0 901  79
##    1  63  42
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#tx d'erreur :}
\NormalTok{tx<-(m[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{m[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{])}\OperatorTok{/}\KeywordTok{length}\NormalTok{(yvalid)}
\NormalTok{tx}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1308756
\end{verbatim}

Le taux d'erreur de ce premier modéle est de 8,5\%.

Deuxième modèle :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn2=}\KeywordTok{neuralnet}\NormalTok{(ytrain}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Xtrain, }\DataTypeTok{hidden=}\DecValTok{10}\NormalTok{, }\DataTypeTok{err.fct=}\StringTok{"ce"}\NormalTok{,}\DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{,}\DataTypeTok{likelihood =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{prob2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nn2,valid)}
\NormalTok{pred2 <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(prob2}\OperatorTok{>}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\NormalTok{m2<-}\KeywordTok{table}\NormalTok{(pred2,yvalid)}
\NormalTok{tx2<-(m2[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{m2[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{])}\OperatorTok{/}\KeywordTok{length}\NormalTok{(yvalid)}
\NormalTok{tx2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1271889
\end{verbatim}

Le modéle deux est plus performant avec le ``likelihood'' (taux d'erreur
= 8,1\% \textless{} 8,5\%).

Troixième modèle : augmentation du nombre de couche de la couche caché
de 10 à 17 (nombre de variable d'entrée)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#nn3=neuralnet(ytrain~., data = Xtrain, hidden=17, err.fct="ce",linear.output = FALSE)}

\CommentTok{#prob3 <- predict(nn3,valid)}
\CommentTok{#pred3 <- ifelse(prob3>0.5, 1, 0)}

\CommentTok{#m3<-table(pred3,yvalid)}
\CommentTok{#tx3<-(m3[1,2]+m3[2,1])/length(yvalid)}
\CommentTok{#tx3}
\end{Highlighting}
\end{Shaded}

Taux d'erreur un peu meilleur (7,9\%\textless8,1\%)

Quatrième modèle : augmentation du nombre de couche de la couche caché
de 17 à 34 (deux fois le nombre de variable d'entrée)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn4=}\KeywordTok{neuralnet}\NormalTok{(ytrain}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Xtrain, }\DataTypeTok{hidden=}\DecValTok{34}\NormalTok{, }\DataTypeTok{err.fct=}\StringTok{"ce"}\NormalTok{,}\DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{)}

\NormalTok{prob4 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nn4,valid)}
\NormalTok{pred4 <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(prob4}\OperatorTok{>}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\NormalTok{m4<-}\KeywordTok{table}\NormalTok{(pred4,yvalid)}
\NormalTok{tx4<-(m4[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{m[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{])}\OperatorTok{/}\KeywordTok{length}\NormalTok{(yvalid)}
\NormalTok{tx4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1317972
\end{verbatim}

Taux d'erreur un peu élevé, donc moins intéressant (7,6\%\textless7,9\%)

On garde la prédiction n°3 pour le nombre de neurones de la couche
caché. Et nous décidons de garder 17 neurones dans la couche caché.

Cinquième modèle : On utilise une couche caché composé de 34 neurones et
on passe l'argument ``likelihood'' à vrai.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn5<-}\KeywordTok{neuralnet}\NormalTok{(ytrain}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Xtrain, }\DataTypeTok{hidden=}\DecValTok{34}\NormalTok{, }\DataTypeTok{err.fct=}\StringTok{"ce"}\NormalTok{,}\DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{likelihood =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{prob5 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nn5,valid)}
\NormalTok{pred5 <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(prob5}\OperatorTok{>}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\NormalTok{m5<-}\KeywordTok{table}\NormalTok{(pred5,yvalid)}
\NormalTok{tx5<-(m5[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{m[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{])}\OperatorTok{/}\KeywordTok{length}\NormalTok{(yvalid)}
\NormalTok{tx5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1345622
\end{verbatim}

Cette fois ci le taux d'erreur est plus élevé (8,0\%)

Nous testons un dernier modéle.

Sixième modèle : Nous augumentons encore une fois le nombre de neurones
de la couche cachée (3 fois le nombre de variable d'entrée)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn6<-}\KeywordTok{neuralnet}\NormalTok{(ytrain}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Xtrain, }\DataTypeTok{hidden=}\DecValTok{51}\NormalTok{, }\DataTypeTok{err.fct=}\StringTok{"ce"}\NormalTok{,}\DataTypeTok{linear.output =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{likelihood =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{prob6 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nn6,valid)}
\NormalTok{pred6 <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(prob6}\OperatorTok{>}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\NormalTok{m6<-}\KeywordTok{table}\NormalTok{(pred6,yvalid)}
\NormalTok{tx6<-(m6[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{m[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{])}\OperatorTok{/}\KeywordTok{length}\NormalTok{(yvalid)}
\NormalTok{tx6}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1308756
\end{verbatim}

Cette fois le taux d'erreur est de 7,8\% ce qui est faiblement plus
élevé que pour le quatrième modèle (7,6\%). Mais ce modèle utilise un
plus grand nombre de neurone ce qui peut engendrer du sur-apprentissage.

Avec la library ``neutralnn'' et la variable cible y correspondant à une
classification binaire, il n'est pas possible de paramétrer le réseau de
neurone à l'infinie. Les paramétrages pour une telle variable sont
faible. Le paramètre ``algorithme'' ne fonctionne qu'avec ``rprop +'',
la fonction d'erreur ne peut pas être modifier et la demande d'une
régression logistique est impossible. Nous aurions pu changer les poids
les variables d'entrée, mais le choix de l'aléatoire nous pariassait
plus juste. Néanmoins, avec les réseaux de neurones que nous avons
construit nous avons un taux d'erreur plutôt faible, ce qui nous
paraissait être un bon résultat.Pour confirmer cette avis, nous décidons
de comparer les courbes ROC des six modèles.

courbe ROC :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_pred =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(pred, pred2,  pred4, pred5, pred6)}
\KeywordTok{colnames}\NormalTok{(all_pred) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"nn"}\NormalTok{,}\StringTok{"nn2"}\NormalTok{,}\StringTok{"nn4"}\NormalTok{,}\StringTok{"nn5"}\NormalTok{,}\StringTok{"nn6"}\NormalTok{)}
\KeywordTok{colAUC}\NormalTok{(all_pred, yvalid, }\DataTypeTok{plotROC =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                nn       nn2       nn4       nn5       nn6
## 0 vs. 1 0.6408774 0.6935419 0.6253344 0.6160497 0.6486575
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rapport_files/figure-latex/unnamed-chunk-12-1.pdf}

A l'aide de la Courbe ROC ainsi que du taux d'erreur, nous considérons
le meilleur réseau de neurone étant le quatrième modèle construit. C'est
a dire que nous utilisons 17 neuronnes caché et que nous plaçons à vrai
l'argument ``likelihood''. C'est avec ce modèle que nous allons vérifier
la bonne prédiction des valeurs test.

\#\#Prédiction de la variable cible y

Représentation graphique du réseau de neurones séléctionné :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(nn4)}
\end{Highlighting}
\end{Shaded}

Voici la distribution de la variable y prédite par le réseau de neurone
:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(nn4,test)}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(prob}\OperatorTok{>}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{pred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## 7       0
## 12      0
## 16      0
## 25      0
## 27      0
## 28      0
## 39      0
## 40      0
## 41      0
## 42      0
## 45      0
## 52      1
## 58      0
## 59      0
## 65      0
## 70      0
## 75      0
## 79      0
## 81      1
## 88      0
## 89      0
## 92      0
## 102     0
## 103     0
## 106     0
## 112     0
## 113     0
## 115     0
## 116     0
## 117     0
## 129     0
## 137     0
## 140     0
## 145     0
## 148     0
## 153     0
## 155     0
## 157     1
## 161     0
## 163     0
## 171     0
## 173     0
## 174     0
## 186     0
## 191     0
## 196     0
## 197     0
## 203     0
## 206     0
## 209     0
## 213     0
## 217     0
## 224     0
## 226     0
## 229     0
## 234     1
## 236     0
## 240     0
## 241     0
## 242     0
## 244     0
## 249     0
## 253     0
## 261     0
## 264     0
## 268     0
## 277     0
## 279     0
## 281     0
## 283     0
## 288     0
## 293     0
## 300     0
## 302     0
## 304     0
## 329     1
## 333     0
## 337     0
## 341     0
## 346     0
## 349     0
## 354     0
## 363     0
## 367     0
## 377     0
## 390     0
## 391     1
## 396     0
## 403     0
## 406     0
## 407     0
## 408     0
## 415     0
## 417     1
## 425     0
## 428     0
## 430     0
## 432     0
## 437     0
## 438     0
## 440     0
## 444     0
## 447     0
## 461     0
## 471     0
## 472     0
## 476     0
## 481     0
## 484     0
## 489     0
## 490     0
## 491     0
## 493     0
## 497     0
## 502     0
## 507     0
## 513     0
## 516     0
## 519     0
## 521     0
## 522     0
## 523     0
## 525     0
## 530     0
## 531     0
## 535     1
## 538     0
## 540     0
## 545     0
## 549     0
## 552     0
## 553     0
## 556     1
## 565     0
## 575     0
## 579     0
## 585     0
## 590     1
## 593     0
## 603     1
## 608     0
## 613     0
## 614     0
## 618     0
## 625     0
## 635     1
## 648     0
## 650     0
## 653     0
## 658     0
## 660     0
## 661     0
## 668     0
## 669     0
## 678     0
## 698     0
## 701     0
## 703     1
## 704     0
## 714     0
## 728     1
## 731     0
## 738     0
## 739     0
## 740     0
## 749     0
## 753     0
## 754     0
## 757     0
## 758     0
## 763     0
## 764     0
## 766     0
## 768     0
## 769     0
## 776     0
## 780     0
## 788     0
## 789     1
## 793     0
## 794     0
## 806     0
## 829     0
## 841     1
## 845     0
## 847     0
## 854     0
## 859     1
## 863     0
## 872     0
## 877     0
## 879     0
## 886     0
## 889     0
## 896     0
## 902     0
## 904     0
## 911     0
## 913     0
## 931     0
## 933     0
## 936     0
## 940     0
## 950     0
## 952     0
## 988     0
## 989     0
## 991     0
## 992     0
## 1008    0
## 1009    0
## 1010    0
## 1012    0
## 1015    0
## 1019    0
## 1028    0
## 1032    0
## 1042    0
## 1047    0
## 1048    1
## 1060    0
## 1070    0
## 1075    0
## 1079    0
## 1087    0
## 1088    0
## 1093    0
## 1097    0
## 1105    0
## 1108    0
## 1117    0
## 1122    0
## 1123    1
## 1125    0
## 1126    0
## 1132    0
## 1133    0
## 1147    0
## 1149    0
## 1156    0
## 1161    0
## 1167    0
## 1172    0
## 1173    0
## 1174    1
## 1177    1
## 1197    0
## 1209    0
## 1210    0
## 1218    0
## 1219    0
## 1220    0
## 1221    0
## 1225    0
## 1230    0
## 1231    0
## 1232    0
## 1235    0
## 1243    0
## 1248    0
## 1250    0
## 1263    0
## 1264    0
## 1271    0
## 1274    0
## 1279    1
## 1282    0
## 1287    0
## 1289    0
## 1299    0
## 1301    1
## 1319    0
## 1327    0
## 1331    1
## 1339    0
## 1342    0
## 1344    0
## 1345    0
## 1346    0
## 1349    0
## 1353    0
## 1359    0
## 1367    0
## 1370    0
## 1373    0
## 1394    0
## 1402    0
## 1412    1
## 1417    0
## 1418    0
## 1421    0
## 1427    0
## 1429    0
## 1436    0
## 1437    0
## 1438    1
## 1442    0
## 1445    1
## 1455    0
## 1458    0
## 1469    0
## 1473    0
## 1484    0
## 1485    0
## 1491    0
## 1495    0
## 1500    0
## 1506    0
## 1509    0
## 1510    0
## 1512    0
## 1519    0
## 1522    1
## 1524    0
## 1526    0
## 1528    0
## 1533    0
## 1538    0
## 1539    0
## 1544    0
## 1546    0
## 1547    0
## 1555    0
## 1559    0
## 1562    0
## 1565    0
## 1570    0
## 1576    0
## 1588    0
## 1605    0
## 1622    0
## 1631    0
## 1632    0
## 1633    0
## 1646    0
## 1649    0
## 1653    0
## 1654    0
## 1656    0
## 1657    0
## 1663    0
## 1664    0
## 1665    0
## 1668    0
## 1673    0
## 1675    0
## 1679    0
## 1680    0
## 1688    0
## 1689    0
## 1692    0
## 1697    0
## 1700    0
## 1719    0
## 1722    0
## 1730    1
## 1734    0
## 1738    0
## 1740    0
## 1750    0
## 1753    1
## 1761    1
## 1763    0
## 1768    1
## 1775    0
## 1782    1
## 1784    0
## 1787    0
## 1792    0
## 1793    0
## 1808    0
## 1830    0
## 1836    0
## 1840    0
## 1845    0
## 1851    0
## 1857    0
## 1860    0
## 1861    0
## 1863    0
## 1869    0
## 1872    0
## 1875    0
## 1880    0
## 1885    0
## 1898    0
## 1900    1
## 1917    0
## 1920    0
## 1924    0
## 1927    0
## 1940    0
## 1941    1
## 1945    0
## 1951    0
## 1953    0
## 1957    1
## 1958    0
## 1966    0
## 1977    0
## 1980    0
## 1985    0
## 1990    0
## 1991    0
## 1993    1
## 2003    0
## 2014    0
## 2042    0
## 2045    0
## 2049    0
## 2061    0
## 2068    0
## 2074    0
## 2076    0
## 2086    0
## 2087    0
## 2088    0
## 2090    0
## 2098    0
## 2100    0
## 2101    0
## 2104    0
## 2108    0
## 2109    0
## 2111    0
## 2112    0
## 2126    0
## 2133    0
## 2141    1
## 2148    0
## 2157    0
## 2161    0
## 2164    0
## 2180    0
## 2182    0
## 2191    0
## 2192    0
## 2193    0
## 2194    0
## 2198    0
## 2200    0
## 2202    0
## 2205    0
## 2211    0
## 2225    0
## 2232    0
## 2233    0
## 2236    0
## 2243    0
## 2247    0
## 2262    0
## 2272    0
## 2276    0
## 2277    0
## 2295    0
## 2305    0
## 2308    0
## 2310    0
## 2323    0
## 2324    0
## 2326    0
## 2329    0
## 2332    0
## 2339    0
## 2345    0
## 2346    0
## 2358    0
## 2384    0
## 2387    0
## 2399    0
## 2401    0
## 2404    0
## 2419    0
## 2424    0
## 2425    0
## 2426    0
## 2431    0
## 2438    0
## 2442    0
## 2458    0
## 2462    0
## 2470    0
## 2496    0
## 2502    0
## 2509    0
## 2518    0
## 2523    0
## 2532    0
## 2534    0
## 2535    0
## 2536    0
## 2540    1
## 2543    0
## 2546    0
## 2549    0
## 2551    0
## 2558    0
## 2562    0
## 2565    0
## 2569    0
## 2571    0
## 2576    0
## 2577    0
## 2579    0
## 2590    0
## 2592    1
## 2597    0
## 2603    0
## 2606    0
## 2615    0
## 2618    0
## 2637    0
## 2639    0
## 2644    0
## 2656    0
## 2665    0
## 2669    0
## 2670    0
## 2682    0
## 2686    0
## 2698    1
## 2703    0
## 2708    0
## 2713    0
## 2714    0
## 2720    0
## 2722    0
## 2726    0
## 2747    1
## 2751    0
## 2767    0
## 2774    0
## 2780    0
## 2781    1
## 2784    0
## 2788    0
## 2790    0
## 2802    0
## 2804    0
## 2809    0
## 2817    0
## 2818    0
## 2824    1
## 2826    0
## 2831    0
## 2839    0
## 2840    0
## 2847    0
## 2852    1
## 2855    0
## 2869    0
## 2871    0
## 2874    0
## 2883    0
## 2884    0
## 2885    1
## 2886    0
## 2892    0
## 2893    0
## 2900    1
## 2901    0
## 2906    0
## 2910    0
## 2915    1
## 2926    0
## 2929    0
## 2932    0
## 2933    0
## 2937    0
## 2946    0
## 2947    0
## 2950    0
## 2958    0
## 2961    0
## 2962    0
## 2963    0
## 2973    0
## 2981    0
## 2985    0
## 2987    0
## 2989    1
## 2998    0
## 3001    0
## 3014    0
## 3022    1
## 3024    1
## 3040    1
## 3043    0
## 3044    0
## 3047    0
## 3053    0
## 3055    0
## 3060    0
## 3063    0
## 3073    0
## 3081    1
## 3084    0
## 3094    0
## 3096    0
## 3099    0
## 3103    0
## 3107    0
## 3108    0
## 3109    0
## 3114    0
## 3119    0
## 3123    0
## 3136    0
## 3139    0
## 3156    0
## 3161    0
## 3162    1
## 3167    1
## 3168    0
## 3169    0
## 3175    0
## 3176    0
## 3180    0
## 3181    0
## 3184    0
## 3186    0
## 3187    0
## 3194    0
## 3195    0
## 3208    0
## 3211    1
## 3215    0
## 3219    0
## 3225    0
## 3227    0
## 3228    0
## 3231    0
## 3233    0
## 3248    0
## 3257    0
## 3258    0
## 3259    0
## 3265    0
## 3288    0
## 3302    1
## 3303    0
## 3316    0
## 3323    0
## 3324    0
## 3331    0
## 3351    0
## 3355    0
## 3358    0
## 3360    0
## 3369    0
## 3374    0
## 3375    1
## 3377    0
## 3381    0
## 3383    0
## 3385    0
## 3389    0
## 3393    0
## 3398    0
## 3399    0
## 3404    0
## 3405    0
## 3414    0
## 3417    0
## 3421    0
## 3424    0
## 3426    0
## 3429    0
## 3431    0
## 3440    0
## 3449    0
## 3450    0
## 3460    0
## 3468    1
## 3471    0
## 3474    0
## 3480    0
## 3490    0
## 3492    0
## 3496    0
## 3503    1
## 3507    0
## 3510    0
## 3515    0
## 3521    0
## 3528    0
## 3529    0
## 3532    0
## 3539    0
## 3542    0
## 3547    1
## 3557    1
## 3558    0
## 3560    0
## 3569    0
## 3572    0
## 3575    0
## 3586    0
## 3589    1
## 3590    1
## 3598    0
## 3599    0
## 3606    0
## 3607    0
## 3612    0
## 3616    0
## 3617    0
## 3623    0
## 3624    0
## 3625    0
## 3637    0
## 3645    1
## 3646    0
## 3654    0
## 3657    0
## 3658    0
## 3659    1
## 3662    0
## 3664    0
## 3678    0
## 3683    0
## 3684    0
## 3689    0
## 3696    0
## 3704    1
## 3709    0
## 3715    0
## 3716    0
## 3717    0
## 3721    1
## 3723    0
## 3725    0
## 3726    0
## 3727    0
## 3729    0
## 3730    0
## 3731    0
## 3744    0
## 3747    0
## 3748    0
## 3751    0
## 3755    1
## 3764    0
## 3774    0
## 3778    0
## 3784    0
## 3787    0
## 3789    0
## 3791    0
## 3793    0
## 3799    0
## 3804    0
## 3813    0
## 3815    0
## 3817    0
## 3818    0
## 3820    0
## 3823    0
## 3833    0
## 3838    0
## 3840    0
## 3842    0
## 3848    0
## 3860    0
## 3861    0
## 3863    0
## 3881    0
## 3883    0
## 3886    0
## 3896    0
## 3897    1
## 3901    0
## 3902    0
## 3908    0
## 3911    0
## 3913    0
## 3914    0
## 3916    0
## 3919    0
## 3924    0
## 3927    0
## 3932    1
## 3936    0
## 3937    1
## 3940    0
## 3942    0
## 3943    0
## 3945    0
## 3949    0
## 3959    0
## 3964    1
## 3967    0
## 3975    0
## 3981    0
## 3982    0
## 3983    0
## 3984    0
## 3991    0
## 4000    0
## 4001    0
## 4002    0
## 4003    0
## 4004    0
## 4008    0
## 4016    0
## 4019    0
## 4023    0
## 4033    0
## 4035    0
## 4036    0
## 4056    0
## 4058    0
## 4061    0
## 4067    0
## 4069    0
## 4072    0
## 4073    0
## 4076    0
## 4080    0
## 4090    0
## 4092    0
## 4104    0
## 4114    0
## 4120    0
## 4127    0
## 4128    0
## 4132    0
## 4147    0
## 4149    0
## 4150    0
## 4151    0
## 4152    0
## 4153    1
## 4156    0
## 4157    0
## 4158    0
## 4166    0
## 4169    0
## 4179    0
## 4180    0
## 4182    0
## 4188    0
## 4192    0
## 4193    0
## 4195    0
## 4208    0
## 4215    0
## 4217    0
## 4221    0
## 4224    1
## 4226    1
## 4235    0
## 4245    0
## 4252    0
## 4256    0
## 4257    0
## 4259    0
## 4261    0
## 4262    0
## 4266    1
## 4275    0
## 4277    0
## 4279    0
## 4287    0
## 4308    0
## 4309    0
## 4311    0
## 4313    1
## 4315    0
## 4320    0
## 4326    0
## 4333    0
## 4334    1
## 4344    0
## 4346    1
## 4350    0
## 4361    0
## 4374    0
## 4375    0
## 4395    0
## 4399    0
## 4401    0
## 4408    0
## 4414    1
## 4416    0
## 4423    1
## 4427    1
## 4436    1
## 4438    0
## 4447    0
## 4449    0
## 4450    1
## 4457    0
## 4466    1
## 4477    0
## 4484    0
## 4487    0
## 4504    0
## 4505    0
## 4506    0
## 4508    0
## 4512    1
## 4513    0
## 4515    0
## 4520    0
\end{verbatim}

Voici la matrice de confusion entre les valeurs prédite et les valeurs
test :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m<-}\KeywordTok{table}\NormalTok{(pred,ytest)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     ytest
## pred   0   1
##    0 747  71
##    1  52  35
\end{verbatim}

Voici le taux d'erreur de cette prédiction :

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#tx d'erreur :}
\NormalTok{tx<-(m[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{m[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{])}\OperatorTok{/}\KeywordTok{length}\NormalTok{(ytest)}
\NormalTok{tx}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1359116
\end{verbatim}

Le taux d'erreur est de 7,1\%.

courbe ROC représentant :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colnames}\NormalTok{(pred)=}\StringTok{"Reseau de neurone"}
\KeywordTok{colAUC}\NormalTok{(pred, ytest, }\DataTypeTok{plotROC =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Reseau de neurone
## 0 vs. 1         0.6325537
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Rapport_files/figure-latex/unnamed-chunk-17-1.pdf}

L'air sous la courbe représentant le reseau de neurones est supérieur à
celle de la courbe représentant l'alétoire.

\end{document}
